
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />
  
  <!-- Licensed under the Apache 2.0 License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/open-sans/stylesheet.css" />
  <!-- Licensed under the SIL Open Font License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/source-serif-pro/source-serif-pro.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap.min.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap-theme.min.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
    <title>Python API Reference &#8212; xgboost 1.0.0-SNAPSHOT documentation</title>
    <link rel="stylesheet" href="../_static/guzzle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://assets.readthedocs.org/static/css/badge_only.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="https://assets.readthedocs.org/static/javascript/readthedocs-doc-embed.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="XGBoost R Package" href="../R-package/index.html" />
    <link rel="prev" title="Python Package Introduction" href="python_intro.html" />
  
   

  
<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="python_api.html" />

<link rel="stylesheet" href="https://assets.readthedocs.org/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="text/javascript" src="../_static/readthedocs-data.js"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = "python/python_api"
READTHEDOCS_DATA['source_suffix'] = ".rst"
</script>

<script type="text/javascript" src="https://assets.readthedocs.org/static/javascript/readthedocs-analytics.js"></script>

<!-- end RTD <extrahead> -->
</head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../R-package/index.html" title="XGBoost R Package"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="python_intro.html" title="Python Package Introduction"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">xgboost 1.0.0-SNAPSHOT documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">XGBoost Python Package</a> &#187;</li> 
      </ul>
    </div>
    <div class="container-wrapper">

      <div id="mobile-toggle">
        <a href="python_api.html#"><span class="glyphicon glyphicon-align-justify" aria-hidden="true"></span></a>
      </div>
  <div id="left-column">
    <div class="sphinxsidebar"><a href="../index.html" class="text-logo">XGBoost</a>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <h2>Table Of Contents</h2>
  </div>
  <div class="sidebar-toc">
    
    
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../build.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Get Started with XGBoost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">XGBoost Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discuss.xgboost.ai">XGBoost User Forum</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu/index.html">GPU support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parameter.html">XGBoost Parameters</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Python package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="python_intro.html">Python Package Introduction</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="python_api.html#">Python API Reference</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/dmlc/xgboost/tree/master/demo/guide-python">Python examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../R-package/index.html">R package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jvm/index.html">JVM package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../julia.html">Julia package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli.html">CLI interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contrib/index.html">Contribute to XGBoost</a></li>
</ul>

    
  </div>
</div>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <div id="main-search">
      <form class="form-inline" action="../search.html" method="GET" role="form">
        <div class="input-group">
          <input name="q" type="text" class="form-control" placeholder="Search...">
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div>
      
    </div>
  </div>
        <div id="right-column">
          
          <div role="navigation" aria-label="breadcrumbs navigation">
            <ol class="breadcrumb">
              <li><a href="../index.html">Docs</a></li>
              
                <li><a href="index.html">XGBoost Python Package</a></li>
              
              <li>Python API Reference</li>
            </ol>
          </div>
          
          <div class="document clearer body">
            
  <div class="section" id="python-api-reference">
<h1>Python API Reference<a class="headerlink" href="python_api.html#python-api-reference" title="Permalink to this headline">¶</a></h1>
<p>This page gives the Python API reference of xgboost, please also refer to Python Package Introduction for more information about python package.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="python_api.html#module-xgboost.core" id="id3">Core Data Structure</a></p></li>
<li><p><a class="reference internal" href="python_api.html#module-xgboost.training" id="id4">Learning API</a></p></li>
<li><p><a class="reference internal" href="python_api.html#module-xgboost.sklearn" id="id5">Scikit-Learn API</a></p></li>
<li><p><a class="reference internal" href="python_api.html#module-xgboost.plotting" id="id6">Plotting API</a></p></li>
<li><p><a class="reference internal" href="python_api.html#callback-api" id="id7">Callback API</a></p></li>
<li><p><a class="reference internal" href="python_api.html#module-xgboost.dask" id="id8">Dask API</a></p></li>
</ul>
</div>
<div class="section" id="module-xgboost.core">
<span id="core-data-structure"></span><h2>Core Data Structure<a class="headerlink" href="python_api.html#module-xgboost.core" title="Permalink to this headline">¶</a></h2>
<p>Core XGBoost Library.</p>
<dl class="class">
<dt id="xgboost.DMatrix">
<em class="property">class </em><code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">DMatrix</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">label=None</em>, <em class="sig-param">missing=None</em>, <em class="sig-param">weight=None</em>, <em class="sig-param">silent=False</em>, <em class="sig-param">feature_names=None</em>, <em class="sig-param">feature_types=None</em>, <em class="sig-param">nthread=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Data Matrix used in XGBoost.</p>
<p>DMatrix is a internal data structure that used by XGBoost
which is optimized for both memory efficiency and training speed.
You can construct DMatrix from numpy.arrays</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>string/numpy.array/scipy.sparse/pd.DataFrame/dt.Frame</em>) – Data source of DMatrix.
When data is string type, it represents the path libsvm format txt file,
or binary file that xgboost can read from.</p></li>
<li><p><strong>label</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em> or </em><em>numpy 1-D array</em><em>, </em><em>optional</em>) – Label of the training data.</p></li>
<li><p><strong>missing</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Value in the data which needs to be present as a missing value. If
None, defaults to np.nan.</p></li>
<li><p><strong>weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em> or </em><em>numpy 1-D array</em><em> , </em><em>optional</em>) – <p>Weight for each instance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For ranking task, weights are per-group.</p>
<p>In ranking task, one weight is assigned to each group (not each data
point). This is because we only care about the relative ordering of
data points within each group, so it doesn’t make sense to assign
weights to individual data points.</p>
</div>
</p></li>
<li><p><strong>silent</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether print messages during construction</p></li>
<li><p><strong>feature_names</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – Set names for features.</p></li>
<li><p><strong>feature_types</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – Set types for features.</p></li>
<li><p><strong>nthread</strong> (<em>integer</em><em>, </em><em>optional</em>) – Number of threads to use for loading data from numpy array. If -1,
uses maximum threads available on the system.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="xgboost.DMatrix.feature_names">
<em class="property">property </em><code class="sig-name descname">feature_names</code><a class="headerlink" href="python_api.html#xgboost.DMatrix.feature_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Get feature names (column labels).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_names</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)">list</a> or <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#None" title="(in Python v3.6)">None</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.feature_types">
<em class="property">property </em><code class="sig-name descname">feature_types</code><a class="headerlink" href="python_api.html#xgboost.DMatrix.feature_types" title="Permalink to this definition">¶</a></dt>
<dd><p>Get feature types (column types).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_types</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)">list</a> or <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#None" title="(in Python v3.6)">None</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.get_base_margin">
<code class="sig-name descname">get_base_margin</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.get_base_margin" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the base margin of the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>base_margin</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.get_float_info">
<code class="sig-name descname">get_float_info</code><span class="sig-paren">(</span><em class="sig-param">field</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.get_float_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Get float property from the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>field</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The field name of the information</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>info</strong> – a numpy array of float information of the data</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.get_label">
<code class="sig-name descname">get_label</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.get_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the label of the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>label</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.get_uint_info">
<code class="sig-name descname">get_uint_info</code><span class="sig-paren">(</span><em class="sig-param">field</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.get_uint_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Get unsigned integer property from the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>field</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The field name of the information</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>info</strong> – a numpy array of unsigned integer information of the data</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.get_weight">
<code class="sig-name descname">get_weight</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.get_weight" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the weight of the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>weight</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.num_col">
<code class="sig-name descname">num_col</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.num_col" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the number of columns (features) in the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>number of columns</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.num_row">
<code class="sig-name descname">num_row</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.num_row" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the number of rows in the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>number of rows</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.save_binary">
<code class="sig-name descname">save_binary</code><span class="sig-paren">(</span><em class="sig-param">fname</em>, <em class="sig-param">silent=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.save_binary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded
by providing the path to <a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">xgboost.DMatrix()</span></code></a> as input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fname</strong> (<em>string</em>) – Name of the output buffer file.</p></li>
<li><p><strong>silent</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em> (</em><em>optional; default: True</em><em>)</em>) – If set, the output is suppressed.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_base_margin">
<code class="sig-name descname">set_base_margin</code><span class="sig-paren">(</span><em class="sig-param">margin</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_base_margin" title="Permalink to this definition">¶</a></dt>
<dd><p>Set base margin of booster to start from.</p>
<p>This can be used to specify a prediction value of
existing model to be base_margin
However, remember margin is needed, instead of transformed prediction
e.g. for logistic regression: need to put in value before logistic transformation
see also example/demo.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>margin</strong> (<em>array like</em>) – Prediction margin of each datapoint</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_float_info">
<code class="sig-name descname">set_float_info</code><span class="sig-paren">(</span><em class="sig-param">field</em>, <em class="sig-param">data</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_float_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Set float type property into the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>field</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The field name of the information</p></li>
<li><p><strong>data</strong> (<em>numpy array</em>) – The array of data to be set</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_float_info_npy2d">
<code class="sig-name descname">set_float_info_npy2d</code><span class="sig-paren">(</span><em class="sig-param">field</em>, <em class="sig-param">data</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_float_info_npy2d" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Set float type property into the DMatrix</dt><dd><p>for numpy 2d array input</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>field</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The field name of the information</p></li>
<li><p><strong>data</strong> (<em>numpy array</em>) – The array of data to be set</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_group">
<code class="sig-name descname">set_group</code><span class="sig-paren">(</span><em class="sig-param">group</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Set group size of DMatrix (used for ranking).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>group</strong> (<em>array like</em>) – Group size of each group</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_label">
<code class="sig-name descname">set_label</code><span class="sig-paren">(</span><em class="sig-param">label</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Set label of dmatrix</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>label</strong> (<em>array like</em>) – The label information to be set into DMatrix</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_label_npy2d">
<code class="sig-name descname">set_label_npy2d</code><span class="sig-paren">(</span><em class="sig-param">label</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_label_npy2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Set label of dmatrix</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>label</strong> (<em>array like</em>) – The label information to be set into DMatrix
from numpy 2D array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_uint_info">
<code class="sig-name descname">set_uint_info</code><span class="sig-paren">(</span><em class="sig-param">field</em>, <em class="sig-param">data</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_uint_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Set uint type property into the DMatrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>field</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The field name of the information</p></li>
<li><p><strong>data</strong> (<em>numpy array</em>) – The array of data to be set</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_weight">
<code class="sig-name descname">set_weight</code><span class="sig-paren">(</span><em class="sig-param">weight</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_weight" title="Permalink to this definition">¶</a></dt>
<dd><p>Set weight of each instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight</strong> (<em>array like</em>) – <p>Weight for each data point</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For ranking task, weights are per-group.</p>
<p>In ranking task, one weight is assigned to each group (not each data
point). This is because we only care about the relative ordering of
data points within each group, so it doesn’t make sense to assign
weights to individual data points.</p>
</div>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.set_weight_npy2d">
<code class="sig-name descname">set_weight_npy2d</code><span class="sig-paren">(</span><em class="sig-param">weight</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.set_weight_npy2d" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Set weight of each instance</dt><dd><p>for numpy 2D array</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight</strong> (<em>array like</em>) – <p>Weight for each data point in numpy 2D array</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For ranking task, weights are per-group.</p>
<p>In ranking task, one weight is assigned to each group (not each data
point). This is because we only care about the relative ordering of
data points within each group, so it doesn’t make sense to assign
weights to individual data points.</p>
</div>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.DMatrix.slice">
<code class="sig-name descname">slice</code><span class="sig-paren">(</span><em class="sig-param">rindex</em>, <em class="sig-param">allow_groups=False</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.DMatrix.slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Slice the DMatrix and return a new DMatrix that only contains <cite>rindex</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rindex</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) – List of indices to be selected.</p></li>
<li><p><strong>allow_groups</strong> (<em>boolean</em>) – Allow slicing of a matrix with a groups attribute</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>res</strong> – A new DMatrix containing only selected indices.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix">DMatrix</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="xgboost.Booster">
<em class="property">class </em><code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">Booster</code><span class="sig-paren">(</span><em class="sig-param">params=None</em>, <em class="sig-param">cache=()</em>, <em class="sig-param">model_file=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>A Booster of XGBoost.</p>
<p>Booster is the model of xgboost, that contains low level routines for
training, prediction and evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Parameters for boosters.</p></li>
<li><p><strong>cache</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) – List of cache items.</p></li>
<li><p><strong>model_file</strong> (<em>string</em>) – Path to the model file.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="xgboost.Booster.attr">
<code class="sig-name descname">attr</code><span class="sig-paren">(</span><em class="sig-param">key</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.attr" title="Permalink to this definition">¶</a></dt>
<dd><p>Get attribute string from the Booster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The key to get attribute from.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>value</strong> – The attribute value of the key, returns None if attribute do not exist.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.attributes">
<code class="sig-name descname">attributes</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.attributes" title="Permalink to this definition">¶</a></dt>
<dd><p>Get attributes stored in the Booster as a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>result</strong> – Returns an empty dict if there’s no attributes.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dictionary of  attribute_name: attribute_value pairs of strings.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.boost">
<code class="sig-name descname">boost</code><span class="sig-paren">(</span><em class="sig-param">dtrain</em>, <em class="sig-param">grad</em>, <em class="sig-param">hess</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.boost" title="Permalink to this definition">¶</a></dt>
<dd><p>Boost the booster for one iteration, with customized gradient
statistics.  Like <code class="xref py py-func docutils literal notranslate"><span class="pre">xgboost.core.Booster.update()</span></code>, this
function should not be called directly by users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtrain</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – The training DMatrix.</p></li>
<li><p><strong>grad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) – The first order of gradient.</p></li>
<li><p><strong>hess</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a>) – The second order of gradient.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.copy">
<code class="sig-name descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy the booster object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>booster</strong> – a copied booster model</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><cite>Booster</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.dump_model">
<code class="sig-name descname">dump_model</code><span class="sig-paren">(</span><em class="sig-param">fout</em>, <em class="sig-param">fmap=''</em>, <em class="sig-param">with_stats=False</em>, <em class="sig-param">dump_format='text'</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.dump_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Dump model into a text or JSON file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fout</strong> (<em>string</em>) – Output file name.</p></li>
<li><p><strong>fmap</strong> (<em>string</em><em>, </em><em>optional</em>) – Name of the file containing feature map names.</p></li>
<li><p><strong>with_stats</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Controls whether the split statistics are output.</p></li>
<li><p><strong>dump_format</strong> (<em>string</em><em>, </em><em>optional</em>) – Format of model dump file. Can be ‘text’ or ‘json’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">name='eval'</em>, <em class="sig-param">iteration=0</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the model on mat.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – The dmatrix storing the input.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>optional</em>) – The name of the dataset.</p></li>
<li><p><strong>iteration</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – The current iteration number.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>result</strong> – Evaluation result string.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.eval_set">
<code class="sig-name descname">eval_set</code><span class="sig-paren">(</span><em class="sig-param">evals</em>, <em class="sig-param">iteration=0</em>, <em class="sig-param">feval=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.eval_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate a set of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>evals</strong> (<em>list of tuples</em><em> (</em><a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a><em>, </em><em>string</em><em>)</em>) – List of items to be evaluated.</p></li>
<li><p><strong>iteration</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Current iteration.</p></li>
<li><p><strong>feval</strong> (<em>function</em>) – Custom evaluation function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>result</strong> – Evaluation result string.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.get_dump">
<code class="sig-name descname">get_dump</code><span class="sig-paren">(</span><em class="sig-param">fmap=''</em>, <em class="sig-param">with_stats=False</em>, <em class="sig-param">dump_format='text'</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.get_dump" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model dump as a list of strings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fmap</strong> (<em>string</em><em>, </em><em>optional</em>) – Name of the file containing feature map names.</p></li>
<li><p><strong>with_stats</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Controls whether the split statistics are output.</p></li>
<li><p><strong>dump_format</strong> (<em>string</em><em>, </em><em>optional</em>) – Format of model dump. Can be ‘text’, ‘json’ or ‘dot’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.get_fscore">
<code class="sig-name descname">get_fscore</code><span class="sig-paren">(</span><em class="sig-param">fmap=''</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.get_fscore" title="Permalink to this definition">¶</a></dt>
<dd><p>Get feature importance of each feature.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature importance is defined only for tree boosters</p>
<p>Feature importance is only defined when the decision tree model is chosen as base
learner (<cite>booster=gbtree</cite>). It is not defined for other base learner types, such
as linear learners (<cite>booster=gblinear</cite>).</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Zero-importance features will not be included</p>
<p>Keep in mind that this function does not include zero-importance feature, i.e.
those features that have not been used in any split conditions.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fmap</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em> (</em><em>optional</em><em>)</em>) – The name of feature map file</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.get_score">
<code class="sig-name descname">get_score</code><span class="sig-paren">(</span><em class="sig-param">fmap=''</em>, <em class="sig-param">importance_type='weight'</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.get_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Get feature importance of each feature.
Importance type can be defined as:</p>
<ul class="simple">
<li><p>‘weight’: the number of times a feature is used to split the data across all trees.</p></li>
<li><p>‘gain’: the average gain across all splits the feature is used in.</p></li>
<li><p>‘cover’: the average coverage across all splits the feature is used in.</p></li>
<li><p>‘total_gain’: the total gain across all splits the feature is used in.</p></li>
<li><p>‘total_cover’: the total coverage across all splits the feature is used in.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature importance is defined only for tree boosters</p>
<p>Feature importance is only defined when the decision tree model is chosen as base
learner (<cite>booster=gbtree</cite>). It is not defined for other base learner types, such
as linear learners (<cite>booster=gblinear</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fmap</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em> (</em><em>optional</em><em>)</em>) – The name of feature map file.</p></li>
<li><p><strong>importance_type</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default 'weight'</em>) – One of the importance types defined above.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.get_split_value_histogram">
<code class="sig-name descname">get_split_value_histogram</code><span class="sig-paren">(</span><em class="sig-param">feature</em>, <em class="sig-param">fmap=''</em>, <em class="sig-param">bins=None</em>, <em class="sig-param">as_pandas=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.get_split_value_histogram" title="Permalink to this definition">¶</a></dt>
<dd><p>Get split value histogram of a feature</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feature</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The name of the feature.</p></li>
<li><p><strong>fmap</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em> (</em><em>optional</em><em>)</em>) – The name of feature map file.</p></li>
<li><p><strong>bin</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>default None</em>) – The maximum number of bins.
Number of bins equals number of unique split values n_unique,
if bins == None or bins &gt; n_unique.</p></li>
<li><p><strong>as_pandas</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>default True</em>) – Return pd.DataFrame when pandas is installed.
If False or pandas is not installed, return numpy ndarray.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>a histogram of used splitting values for the specified feature</em></p></li>
<li><p><em>either as numpy array or pandas DataFrame.</em></p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the model from a file.</p>
<p>The model is loaded from an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature_names) will not be loaded.
To preserve all attributes, pickle the Booster object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em><em> or </em><em>a memory buffer</em>) – Input file name or memory buffer(see also save_raw)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.load_rabit_checkpoint">
<code class="sig-name descname">load_rabit_checkpoint</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.load_rabit_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the model by load from rabit checkpoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>version</strong> – The version number of the model.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">output_margin=False</em>, <em class="sig-param">ntree_limit=0</em>, <em class="sig-param">pred_leaf=False</em>, <em class="sig-param">pred_contribs=False</em>, <em class="sig-param">approx_contribs=False</em>, <em class="sig-param">pred_interactions=False</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict with data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe.</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">bst.copy()</span></code> to make copies
of model object and then call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">predict()</span></code> with DART booster</p>
<p>If the booster object is DART type, <code class="docutils literal notranslate"><span class="pre">predict()</span></code> will perform dropouts, i.e. only
some of the trees will be evaluated. This will produce incorrect results if <code class="docutils literal notranslate"><span class="pre">data</span></code> is
not the training data. To obtain correct results on test sets, set <code class="docutils literal notranslate"><span class="pre">ntree_limit</span></code> to
a nonzero value, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">bst</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">,</span> <span class="n">ntree_limit</span><span class="o">=</span><span class="n">num_round</span><span class="p">)</span>
</pre></div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – The dmatrix storing the input.</p></li>
<li><p><strong>output_margin</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to output the raw untransformed margin value.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to 0 (use all trees).</p></li>
<li><p><strong>pred_leaf</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this option is on, the output will be a matrix of (nsample, ntrees)
with each record indicating the predicted leaf index of each sample in each tree.
Note that the leaf index of a tree is unique per tree, so you may find leaf 1
in both tree 1 and tree 0.</p></li>
<li><p><strong>pred_contribs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True the output will be a matrix of size (nsample, nfeats + 1)
with each record indicating the feature contributions (SHAP values) for that
prediction. The sum of all feature contributions is equal to the raw untransformed
margin value of the prediction. Note the final column is the bias term.</p></li>
<li><p><strong>approx_contribs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Approximate the contributions of each feature</p></li>
<li><p><strong>pred_interactions</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True the output will be a matrix of size (nsample, nfeats + 1, nfeats + 1)
indicating the SHAP interaction values for each pair of features. The sum of each
row (or column) of the interaction values equals the corresponding SHAP value (from
pred_contribs), and the sum of the entire matrix equals the raw untransformed margin
value of the prediction. Note the last row and column correspond to the bias term.</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to a file.</p>
<p>The model is saved in an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature_names) will not be saved.
To preserve all attributes, pickle the Booster object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em>) – Output file name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.save_rabit_checkpoint">
<code class="sig-name descname">save_rabit_checkpoint</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.save_rabit_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the current booster to rabit checkpoint.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.save_raw">
<code class="sig-name descname">save_raw</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.save_raw" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to a in memory buffer representation</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>a in memory buffer representation of the model</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.set_attr">
<code class="sig-name descname">set_attr</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.set_attr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the attribute of the Booster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>**kwargs</strong> – The attributes to set. Setting a value to None deletes an attribute.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.set_param">
<code class="sig-name descname">set_param</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">value=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.set_param" title="Permalink to this definition">¶</a></dt>
<dd><p>Set parameters into the Booster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>dict/list/str</em>) – list of key,value pairs, dict of key to value or simply str key</p></li>
<li><p><strong>value</strong> (<em>optional</em>) – value of the specified parameter, when params is str key</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.trees_to_dataframe">
<code class="sig-name descname">trees_to_dataframe</code><span class="sig-paren">(</span><em class="sig-param">fmap=''</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.trees_to_dataframe" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse a boosted tree model text dump into a pandas DataFrame structure.</p>
<p>This feature is only defined when the decision tree model is chosen as base
learner (<cite>booster in {gbtree, dart}</cite>). It is not defined for other base learner
types, such as linear learners (<cite>booster=gblinear</cite>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fmap</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em> (</em><em>optional</em><em>)</em>) – The name of feature map file.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.Booster.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">dtrain</em>, <em class="sig-param">iteration</em>, <em class="sig-param">fobj=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.Booster.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update for one iteration, with objective function calculated
internally.  This function should not be called directly by users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtrain</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – Training data.</p></li>
<li><p><strong>iteration</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Current iteration number.</p></li>
<li><p><strong>fobj</strong> (<em>function</em>) – Customized objective function.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-xgboost.training">
<span id="learning-api"></span><h2>Learning API<a class="headerlink" href="python_api.html#module-xgboost.training" title="Permalink to this headline">¶</a></h2>
<p>Training Library containing training routines.</p>
<dl class="function">
<dt id="xgboost.train">
<code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">dtrain</em>, <em class="sig-param">num_boost_round=10</em>, <em class="sig-param">evals=()</em>, <em class="sig-param">obj=None</em>, <em class="sig-param">feval=None</em>, <em class="sig-param">maximize=False</em>, <em class="sig-param">early_stopping_rounds=None</em>, <em class="sig-param">evals_result=None</em>, <em class="sig-param">verbose_eval=True</em>, <em class="sig-param">xgb_model=None</em>, <em class="sig-param">callbacks=None</em>, <em class="sig-param">learning_rates=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a booster with given parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Booster params.</p></li>
<li><p><strong>dtrain</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – Data to be trained.</p></li>
<li><p><strong>num_boost_round</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of boosting iterations.</p></li>
<li><p><strong>evals</strong> (<em>list of pairs</em><em> (</em><a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a><em>, </em><em>string</em><em>)</em>) – List of validation sets for which metrics will evaluated during training.
Validation metrics will help us track the performance of the model.</p></li>
<li><p><strong>obj</strong> (<em>function</em>) – Customized objective function.</p></li>
<li><p><strong>feval</strong> (<em>function</em>) – Customized evaluation function.</p></li>
<li><p><strong>maximize</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to maximize feval.</p></li>
<li><p><strong>early_stopping_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Activates early stopping. Validation metric needs to improve at least once in
every <strong>early_stopping_rounds</strong> round(s) to continue training.
Requires at least one item in <strong>evals</strong>.
The method returns the model from the last iteration (not the best one).
If there’s more than one item in <strong>evals</strong>, the last entry will be used
for early stopping.
If there’s more than one metric in the <strong>eval_metric</strong> parameter given in
<strong>params</strong>, the last metric will be used for early stopping.
If early stopping occurs, the model will have three additional fields:
<code class="docutils literal notranslate"><span class="pre">bst.best_score</span></code>, <code class="docutils literal notranslate"><span class="pre">bst.best_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">bst.best_ntree_limit</span></code>.
(Use <code class="docutils literal notranslate"><span class="pre">bst.best_ntree_limit</span></code> to get the correct value if
<code class="docutils literal notranslate"><span class="pre">num_parallel_tree</span></code> and/or <code class="docutils literal notranslate"><span class="pre">num_class</span></code> appears in the parameters)</p></li>
<li><p><strong>evals_result</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – <p>This dictionary stores the evaluation results of all the items in watchlist.</p>
<p>Example: with a watchlist containing
<code class="docutils literal notranslate"><span class="pre">[(dtest,'eval'),</span> <span class="pre">(dtrain,'train')]</span></code> and
a parameter containing <code class="docutils literal notranslate"><span class="pre">('eval_metric':</span> <span class="pre">'logloss')</span></code>,
the <strong>evals_result</strong> returns</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.48253&#39;</span><span class="p">,</span> <span class="s1">&#39;0.35953&#39;</span><span class="p">]},</span>
 <span class="s1">&#39;eval&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.480385&#39;</span><span class="p">,</span> <span class="s1">&#39;0.357756&#39;</span><span class="p">]}}</span>
</pre></div>
</div>
</p></li>
<li><p><strong>verbose_eval</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Requires at least one item in <strong>evals</strong>.
If <strong>verbose_eval</strong> is True then the evaluation metric on the validation set is
printed at each boosting stage.
If <strong>verbose_eval</strong> is an integer then the evaluation metric on the validation set
is printed at every given <strong>verbose_eval</strong> boosting stage. The last boosting stage
/ the boosting stage found by using <strong>early_stopping_rounds</strong> is also printed.
Example: with <code class="docutils literal notranslate"><span class="pre">verbose_eval=4</span></code> and at least one item in <strong>evals</strong>, an evaluation metric
is printed every 4 boosting stages, instead of every boosting stage.</p></li>
<li><p><strong>learning_rates</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em> or </em><em>function</em><em> (</em><em>deprecated - use callback API instead</em><em>)</em>) – List of learning rate for each boosting round
or a customized function that calculates eta in terms of
current number of round and the total number of boosting round (e.g. yields
learning rate decay)</p></li>
<li><p><strong>xgb_model</strong> (<em>file name of stored xgb model</em><em> or </em><em>'Booster' instance</em>) – Xgb model to be loaded before training (allows training continuation).</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback functions</em>) – <p>List of callback functions that are applied at end of each iteration.
It is possible to use predefined callbacks by using
<a class="reference internal" href="python_api.html#callback-api"><span class="std std-ref">Callback API</span></a>.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">reset_learning_rate</span><span class="p">(</span><span class="n">custom_rates</span><span class="p">)]</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Booster</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>a trained booster model</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.cv">
<code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">cv</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">dtrain</em>, <em class="sig-param">num_boost_round=10</em>, <em class="sig-param">nfold=3</em>, <em class="sig-param">stratified=False</em>, <em class="sig-param">folds=None</em>, <em class="sig-param">metrics=()</em>, <em class="sig-param">obj=None</em>, <em class="sig-param">feval=None</em>, <em class="sig-param">maximize=False</em>, <em class="sig-param">early_stopping_rounds=None</em>, <em class="sig-param">fpreproc=None</em>, <em class="sig-param">as_pandas=True</em>, <em class="sig-param">verbose_eval=None</em>, <em class="sig-param">show_stdv=True</em>, <em class="sig-param">seed=0</em>, <em class="sig-param">callbacks=None</em>, <em class="sig-param">shuffle=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-validation with given parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Booster params.</p></li>
<li><p><strong>dtrain</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – Data to be trained.</p></li>
<li><p><strong>num_boost_round</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of boosting iterations.</p></li>
<li><p><strong>nfold</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of folds in CV.</p></li>
<li><p><strong>stratified</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Perform stratified sampling.</p></li>
<li><p><strong>folds</strong> (<em>a KFold</em><em> or </em><em>StratifiedKFold instance</em><em> or </em><em>list of fold indices</em>) – Sklearn KFolds or StratifiedKFolds object.
Alternatively may explicitly pass sample indices for each fold.
For <code class="docutils literal notranslate"><span class="pre">n</span></code> folds, <strong>folds</strong> should be a length <code class="docutils literal notranslate"><span class="pre">n</span></code> list of tuples.
Each tuple is <code class="docutils literal notranslate"><span class="pre">(in,out)</span></code> where <code class="docutils literal notranslate"><span class="pre">in</span></code> is a list of indices to be used
as the training samples for the <code class="docutils literal notranslate"><span class="pre">n</span></code> th fold and <code class="docutils literal notranslate"><span class="pre">out</span></code> is a list of
indices to be used as the testing samples for the <code class="docutils literal notranslate"><span class="pre">n</span></code> th fold.</p></li>
<li><p><strong>metrics</strong> (<em>string</em><em> or </em><em>list of strings</em>) – Evaluation metrics to be watched in CV.</p></li>
<li><p><strong>obj</strong> (<em>function</em>) – Custom objective function.</p></li>
<li><p><strong>feval</strong> (<em>function</em>) – Custom evaluation function.</p></li>
<li><p><strong>maximize</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to maximize feval.</p></li>
<li><p><strong>early_stopping_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Activates early stopping. Cross-Validation metric (average of validation
metric computed over CV folds) needs to improve at least once in
every <strong>early_stopping_rounds</strong> round(s) to continue training.
The last entry in the evaluation history will represent the best iteration.
If there’s more than one metric in the <strong>eval_metric</strong> parameter given in
<strong>params</strong>, the last metric will be used for early stopping.</p></li>
<li><p><strong>fpreproc</strong> (<em>function</em>) – Preprocessing function that takes (dtrain, dtest, param) and returns
transformed versions of those.</p></li>
<li><p><strong>as_pandas</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>default True</em>) – Return pd.DataFrame when pandas is installed.
If False or pandas is not installed, return np.ndarray</p></li>
<li><p><strong>verbose_eval</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3.6/library/constants.html#None" title="(in Python v3.6)"><em>None</em></a><em>, </em><em>default None</em>) – Whether to display the progress. If None, progress will be displayed
when np.ndarray is returned. If True, progress will be displayed at
boosting stage. If an integer is given, progress will be displayed
at every given <cite>verbose_eval</cite> boosting stage.</p></li>
<li><p><strong>show_stdv</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>default True</em>) – Whether to display the standard deviation in progress.
Results are not affected, and always contains std.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Seed used to generate the folds (passed to numpy.random.seed).</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback functions</em>) – <p>List of callback functions that are applied at end of each iteration.
It is possible to use predefined callbacks by using
<a class="reference internal" href="python_api.html#callback-api"><span class="std std-ref">Callback API</span></a>.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">reset_learning_rate</span><span class="p">(</span><span class="n">custom_rates</span><span class="p">)]</span>
</pre></div>
</div>
</p></li>
<li><p><strong>shuffle</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Shuffle data before creating folds.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>evaluation history</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)">list</a>(string)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-xgboost.sklearn">
<span id="scikit-learn-api"></span><h2>Scikit-Learn API<a class="headerlink" href="python_api.html#module-xgboost.sklearn" title="Permalink to this headline">¶</a></h2>
<p>Scikit-Learn Wrapper interface for XGBoost.</p>
<dl class="class">
<dt id="xgboost.XGBRegressor">
<em class="property">class </em><code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">XGBRegressor</code><span class="sig-paren">(</span><em class="sig-param">max_depth=3</em>, <em class="sig-param">learning_rate=0.1</em>, <em class="sig-param">n_estimators=100</em>, <em class="sig-param">verbosity=1</em>, <em class="sig-param">silent=None</em>, <em class="sig-param">objective='reg:squarederror'</em>, <em class="sig-param">booster='gbtree'</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">nthread=None</em>, <em class="sig-param">gamma=0</em>, <em class="sig-param">min_child_weight=1</em>, <em class="sig-param">max_delta_step=0</em>, <em class="sig-param">subsample=1</em>, <em class="sig-param">colsample_bytree=1</em>, <em class="sig-param">colsample_bylevel=1</em>, <em class="sig-param">colsample_bynode=1</em>, <em class="sig-param">reg_alpha=0</em>, <em class="sig-param">reg_lambda=1</em>, <em class="sig-param">scale_pos_weight=1</em>, <em class="sig-param">base_score=0.5</em>, <em class="sig-param">random_state=0</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">missing=None</em>, <em class="sig-param">importance_type='gain'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xgboost.sklearn.XGBModel</span></code>, <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Implementation of the scikit-learn API for XGBoost regression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_depth</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum tree depth for base learners.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Boosting learning rate (xgb’s “eta”)</p></li>
<li><p><strong>n_estimators</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of trees to fit.</p></li>
<li><p><strong>verbosity</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).</p></li>
<li><p><strong>silent</strong> (<em>boolean</em>) – Whether to print messages while running boosting. Deprecated. Use verbosity instead.</p></li>
<li><p><strong>objective</strong> (<em>string</em><em> or </em><em>callable</em>) – Specify the learning task and the corresponding learning objective or
a custom objective function to be used (see note below).</p></li>
<li><p><strong>booster</strong> (<em>string</em>) – Specify which booster to use: gbtree, gblinear or dart.</p></li>
<li><p><strong>nthread</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (Deprecated, please use <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>)</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (replaces <code class="docutils literal notranslate"><span class="pre">nthread</span></code>)</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Minimum loss reduction required to make a further partition on a leaf node of the tree.</p></li>
<li><p><strong>min_child_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Minimum sum of instance weight(hessian) needed in a child.</p></li>
<li><p><strong>max_delta_step</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum delta step we allow each tree’s weight estimation to be.</p></li>
<li><p><strong>subsample</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of the training instance.</p></li>
<li><p><strong>colsample_bytree</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns when constructing each tree.</p></li>
<li><p><strong>colsample_bylevel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each level.</p></li>
<li><p><strong>colsample_bynode</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each split.</p></li>
<li><p><strong>reg_alpha</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's alpha</em><em>)</em>) – L1 regularization term on weights</p></li>
<li><p><strong>reg_lambda</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's lambda</em><em>)</em>) – L2 regularization term on weights</p></li>
<li><p><strong>scale_pos_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Balancing of positive and negative weights.</p></li>
<li><p><strong>base_score</strong> – The initial prediction score of all instances, global bias.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (Deprecated, please use random_state)</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (replaces seed)</p></li>
<li><p><strong>missing</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Value in the data which needs to be present as a missing value. If
None, defaults to np.nan.</p></li>
<li><p><strong>importance_type</strong> (<em>string</em><em>, </em><em>default &quot;gain&quot;</em>) – The feature importance type for the feature_importances_ property:
either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.</p></li>
<li><p><strong>**kwargs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>Keyword arguments for XGBoost Booster object.  Full documentation of parameters can
be found here: <a class="reference external" href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst">https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst</a>.
Attempting to set a parameter via the constructor args and **kwargs dict simultaneously
will result in a TypeError.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>**kwargs unsupported by scikit-learn</p>
<p>**kwargs is unsupported by scikit-learn.  We do not guarantee that parameters
passed via this argument will interact properly with scikit-learn.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A custom objective function can be provided for the <code class="docutils literal notranslate"><span class="pre">objective</span></code>
parameter. In this case, it should have the signature
<code class="docutils literal notranslate"><span class="pre">objective(y_true,</span> <span class="pre">y_pred)</span> <span class="pre">-&gt;</span> <span class="pre">grad,</span> <span class="pre">hess</span></code>:</p>
<dl class="simple">
<dt>y_true: array_like of shape [n_samples]</dt><dd><p>The target values</p>
</dd>
<dt>y_pred: array_like of shape [n_samples]</dt><dd><p>The predicted values</p>
</dd>
<dt>grad: array_like of shape [n_samples]</dt><dd><p>The value of the gradient for each sample point.</p>
</dd>
<dt>hess: array_like of shape [n_samples]</dt><dd><p>The value of the second derivative for each sample point</p>
</dd>
</dl>
</div>
<dl class="method">
<dt id="xgboost.XGBRegressor.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">ntree_limit=0</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the predicted leaf every tree for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em><em>, </em><em>shape=</em><em>[</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Input features matrix.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to 0 (use all trees).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X and for each tree, return the index of the
leaf x ends up in. Leaves are numbered within
<code class="docutils literal notranslate"><span class="pre">[0;</span> <span class="pre">2**(self.max_depth+1))</span></code>, possibly with gaps in the numbering.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array_like, shape=[n_samples, n_trees]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.coef_">
<em class="property">property </em><code class="sig-name descname">coef_</code><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Coefficients are defined only for linear learners</p>
<p>Coefficients are only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>coef_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes,</span> <span class="pre">n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.evals_result">
<code class="sig-name descname">evals_result</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.evals_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the evaluation results.</p>
<p>If <strong>eval_set</strong> is passed to the <cite>fit</cite> function, you can call
<code class="docutils literal notranslate"><span class="pre">evals_result()</span></code> to get evaluation results for all passed <strong>eval_sets</strong>.
When <strong>eval_metric</strong> is also passed to the <cite>fit</cite> function, the
<strong>evals_result</strong> will contain the <strong>eval_metrics</strong> passed to the <cite>fit</cite> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>evals_result</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dictionary</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBModel</span><span class="p">(</span><span class="o">**</span><span class="n">param_dist</span><span class="p">)</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)],</span>
        <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">evals_result</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">evals_result</span><span class="p">()</span>
</pre></div>
</div>
<p>The variable <strong>evals_result</strong> will contain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;validation_0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.604835&#39;</span><span class="p">,</span> <span class="s1">&#39;0.531479&#39;</span><span class="p">]},</span>
<span class="s1">&#39;validation_1&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.41965&#39;</span><span class="p">,</span> <span class="s1">&#39;0.17686&#39;</span><span class="p">]}}</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.feature_importances_">
<em class="property">property </em><code class="sig-name descname">feature_importances_</code><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature importances property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature importance is defined only for tree boosters</p>
<p>Feature importance is only defined when the decision tree model is chosen as base
learner (<cite>booster=gbtree</cite>). It is not defined for other base learner types, such
as linear learners (<cite>booster=gblinear</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_importances_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">eval_metric=None</em>, <em class="sig-param">early_stopping_rounds=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">xgb_model=None</em>, <em class="sig-param">sample_weight_eval_set=None</em>, <em class="sig-param">callbacks=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit gradient boosting model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em>) – Feature matrix</p></li>
<li><p><strong>y</strong> (<em>array_like</em>) – Labels</p></li>
<li><p><strong>sample_weight</strong> (<em>array_like</em>) – instance weights</p></li>
<li><p><strong>eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of (X, y) tuple pairs to use as validation sets, for which
metrics will be computed.
Validation metrics will help us track the performance of the model.</p></li>
<li><p><strong>sample_weight_eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of the form [L_1, L_2, …, L_n], where each L_i is a list of
instance weights on the i-th validation set.</p></li>
<li><p><strong>eval_metric</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>list of str</em><em>, or </em><em>callable</em><em>, </em><em>optional</em>) – If a str, should be a built-in evaluation metric to use. See
doc/parameter.rst.
If a list of str, should be the list of multiple built-in evaluation metrics
to use.
If callable, a custom evaluation metric. The call
signature is <code class="docutils literal notranslate"><span class="pre">func(y_predicted,</span> <span class="pre">y_true)</span></code> where <code class="docutils literal notranslate"><span class="pre">y_true</span></code> will be a
DMatrix object such that you may need to call the <code class="docutils literal notranslate"><span class="pre">get_label</span></code>
method. It must return a str, value pair where the str is a name
for the evaluation and value is the value of the evaluation
function. The callable custom objective is always minimized.</p></li>
<li><p><strong>early_stopping_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Activates early stopping. Validation metric needs to improve at least once in
every <strong>early_stopping_rounds</strong> round(s) to continue training.
Requires at least one item in <strong>eval_set</strong>.
The method returns the model from the last iteration (not the best one).
If there’s more than one item in <strong>eval_set</strong>, the last entry will be used
for early stopping.
If there’s more than one metric in <strong>eval_metric</strong>, the last metric will be
used for early stopping.
If early stopping occurs, the model will have three additional fields:
<code class="docutils literal notranslate"><span class="pre">clf.best_score</span></code>, <code class="docutils literal notranslate"><span class="pre">clf.best_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">clf.best_ntree_limit</span></code>.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <cite>verbose</cite> and an evaluation set is used, writes the evaluation
metric measured on the validation set to stderr.</p></li>
<li><p><strong>xgb_model</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be
loaded before training (allows training continuation).</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback functions</em>) – <p>List of callback functions that are applied at end of each iteration.
It is possible to use predefined callbacks by using <a class="reference internal" href="python_api.html#callback-api"><span class="std std-ref">Callback API</span></a>.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">reset_learning_rate</span><span class="p">(</span><span class="n">custom_rates</span><span class="p">)]</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.get_booster">
<code class="sig-name descname">get_booster</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.get_booster" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the underlying xgboost Booster of this model.</p>
<p>This will raise an exception when fit was not called</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>booster</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>a xgboost booster of underlying model</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.get_num_boosting_rounds">
<code class="sig-name descname">get_num_boosting_rounds</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.get_num_boosting_rounds" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the number of xgboost boosting rounds.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=False</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.get_xgb_params">
<code class="sig-name descname">get_xgb_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.get_xgb_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get xgboost type parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.intercept_">
<em class="property">property </em><code class="sig-name descname">intercept_</code><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (bias) property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Intercept is defined only for linear learners</p>
<p>Intercept (bias) is only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>intercept_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the model from a file.</p>
<p>The model is loaded from an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em><em> or </em><em>a memory buffer</em>) – Input file name or memory buffer(see also save_raw)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">output_margin=False</em>, <em class="sig-param">ntree_limit=None</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict with <cite>data</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe.</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">xgb.copy()</span></code> to make copies
of model object and then call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">predict()</span></code> with DART booster</p>
<p>If the booster object is DART type, <code class="docutils literal notranslate"><span class="pre">predict()</span></code> will perform dropouts, i.e. only
some of the trees will be evaluated. This will produce incorrect results if <code class="docutils literal notranslate"><span class="pre">data</span></code> is
not the training data. To obtain correct results on test sets, set <code class="docutils literal notranslate"><span class="pre">ntree_limit</span></code> to
a nonzero value, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">bst</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">,</span> <span class="n">ntree_limit</span><span class="o">=</span><span class="n">num_round</span><span class="p">)</span>
</pre></div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>numpy.array/scipy.sparse</em>) – Data to predict with</p></li>
<li><p><strong>output_margin</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to output the raw untransformed margin value.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to best_ntree_limit if defined
(i.e. it has been trained with early stopping), otherwise 0 (use all trees).</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to a file.</p>
<p>The model is saved in an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em>) – Output file name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRegressor.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
Modification of the sklearn method to allow unknown kwargs. This allows using
the full range of xgboost parameters that are not defined as member variables
in sklearn grid search.
:returns:
:rtype: self</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="xgboost.XGBClassifier">
<em class="property">class </em><code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">XGBClassifier</code><span class="sig-paren">(</span><em class="sig-param">max_depth=3</em>, <em class="sig-param">learning_rate=0.1</em>, <em class="sig-param">n_estimators=100</em>, <em class="sig-param">verbosity=1</em>, <em class="sig-param">silent=None</em>, <em class="sig-param">objective='binary:logistic'</em>, <em class="sig-param">booster='gbtree'</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">nthread=None</em>, <em class="sig-param">gamma=0</em>, <em class="sig-param">min_child_weight=1</em>, <em class="sig-param">max_delta_step=0</em>, <em class="sig-param">subsample=1</em>, <em class="sig-param">colsample_bytree=1</em>, <em class="sig-param">colsample_bylevel=1</em>, <em class="sig-param">colsample_bynode=1</em>, <em class="sig-param">reg_alpha=0</em>, <em class="sig-param">reg_lambda=1</em>, <em class="sig-param">scale_pos_weight=1</em>, <em class="sig-param">base_score=0.5</em>, <em class="sig-param">random_state=0</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">missing=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xgboost.sklearn.XGBModel</span></code>, <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Implementation of the scikit-learn API for XGBoost classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_depth</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum tree depth for base learners.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Boosting learning rate (xgb’s “eta”)</p></li>
<li><p><strong>n_estimators</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of trees to fit.</p></li>
<li><p><strong>verbosity</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).</p></li>
<li><p><strong>silent</strong> (<em>boolean</em>) – Whether to print messages while running boosting. Deprecated. Use verbosity instead.</p></li>
<li><p><strong>objective</strong> (<em>string</em><em> or </em><em>callable</em>) – Specify the learning task and the corresponding learning objective or
a custom objective function to be used (see note below).</p></li>
<li><p><strong>booster</strong> (<em>string</em>) – Specify which booster to use: gbtree, gblinear or dart.</p></li>
<li><p><strong>nthread</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (Deprecated, please use <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>)</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (replaces <code class="docutils literal notranslate"><span class="pre">nthread</span></code>)</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Minimum loss reduction required to make a further partition on a leaf node of the tree.</p></li>
<li><p><strong>min_child_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Minimum sum of instance weight(hessian) needed in a child.</p></li>
<li><p><strong>max_delta_step</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum delta step we allow each tree’s weight estimation to be.</p></li>
<li><p><strong>subsample</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of the training instance.</p></li>
<li><p><strong>colsample_bytree</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns when constructing each tree.</p></li>
<li><p><strong>colsample_bylevel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each level.</p></li>
<li><p><strong>colsample_bynode</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each split.</p></li>
<li><p><strong>reg_alpha</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's alpha</em><em>)</em>) – L1 regularization term on weights</p></li>
<li><p><strong>reg_lambda</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's lambda</em><em>)</em>) – L2 regularization term on weights</p></li>
<li><p><strong>scale_pos_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Balancing of positive and negative weights.</p></li>
<li><p><strong>base_score</strong> – The initial prediction score of all instances, global bias.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (Deprecated, please use random_state)</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (replaces seed)</p></li>
<li><p><strong>missing</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Value in the data which needs to be present as a missing value. If
None, defaults to np.nan.</p></li>
<li><p><strong>importance_type</strong> (<em>string</em><em>, </em><em>default &quot;gain&quot;</em>) – The feature importance type for the feature_importances_ property:
either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.</p></li>
<li><p><strong>**kwargs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>Keyword arguments for XGBoost Booster object.  Full documentation of parameters can
be found here: <a class="reference external" href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst">https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst</a>.
Attempting to set a parameter via the constructor args and **kwargs dict simultaneously
will result in a TypeError.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>**kwargs unsupported by scikit-learn</p>
<p>**kwargs is unsupported by scikit-learn.  We do not guarantee that parameters
passed via this argument will interact properly with scikit-learn.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A custom objective function can be provided for the <code class="docutils literal notranslate"><span class="pre">objective</span></code>
parameter. In this case, it should have the signature
<code class="docutils literal notranslate"><span class="pre">objective(y_true,</span> <span class="pre">y_pred)</span> <span class="pre">-&gt;</span> <span class="pre">grad,</span> <span class="pre">hess</span></code>:</p>
<dl class="simple">
<dt>y_true: array_like of shape [n_samples]</dt><dd><p>The target values</p>
</dd>
<dt>y_pred: array_like of shape [n_samples]</dt><dd><p>The predicted values</p>
</dd>
<dt>grad: array_like of shape [n_samples]</dt><dd><p>The value of the gradient for each sample point.</p>
</dd>
<dt>hess: array_like of shape [n_samples]</dt><dd><p>The value of the second derivative for each sample point</p>
</dd>
</dl>
</div>
<dl class="method">
<dt id="xgboost.XGBClassifier.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">ntree_limit=0</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the predicted leaf every tree for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em><em>, </em><em>shape=</em><em>[</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Input features matrix.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to 0 (use all trees).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X and for each tree, return the index of the
leaf x ends up in. Leaves are numbered within
<code class="docutils literal notranslate"><span class="pre">[0;</span> <span class="pre">2**(self.max_depth+1))</span></code>, possibly with gaps in the numbering.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array_like, shape=[n_samples, n_trees]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.coef_">
<em class="property">property </em><code class="sig-name descname">coef_</code><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Coefficients are defined only for linear learners</p>
<p>Coefficients are only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>coef_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes,</span> <span class="pre">n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.evals_result">
<code class="sig-name descname">evals_result</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.evals_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the evaluation results.</p>
<p>If <strong>eval_set</strong> is passed to the <cite>fit</cite> function, you can call
<code class="docutils literal notranslate"><span class="pre">evals_result()</span></code> to get evaluation results for all passed <strong>eval_sets</strong>.
When <strong>eval_metric</strong> is also passed to the <cite>fit</cite> function, the
<strong>evals_result</strong> will contain the <strong>eval_metrics</strong> passed to the <cite>fit</cite> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>evals_result</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dictionary</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">param_dist</span><span class="p">)</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)],</span>
        <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">evals_result</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">evals_result</span><span class="p">()</span>
</pre></div>
</div>
<p>The variable <strong>evals_result</strong> will contain</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;validation_0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.604835&#39;</span><span class="p">,</span> <span class="s1">&#39;0.531479&#39;</span><span class="p">]},</span>
<span class="s1">&#39;validation_1&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.41965&#39;</span><span class="p">,</span> <span class="s1">&#39;0.17686&#39;</span><span class="p">]}}</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.feature_importances_">
<em class="property">property </em><code class="sig-name descname">feature_importances_</code><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature importances property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature importance is defined only for tree boosters</p>
<p>Feature importance is only defined when the decision tree model is chosen as base
learner (<cite>booster=gbtree</cite>). It is not defined for other base learner types, such
as linear learners (<cite>booster=gblinear</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_importances_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">eval_metric=None</em>, <em class="sig-param">early_stopping_rounds=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">xgb_model=None</em>, <em class="sig-param">sample_weight_eval_set=None</em>, <em class="sig-param">callbacks=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit gradient boosting classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em>) – Feature matrix</p></li>
<li><p><strong>y</strong> (<em>array_like</em>) – Labels</p></li>
<li><p><strong>sample_weight</strong> (<em>array_like</em>) – instance weights</p></li>
<li><p><strong>eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of (X, y) tuple pairs to use as validation sets, for which
metrics will be computed.
Validation metrics will help us track the performance of the model.</p></li>
<li><p><strong>sample_weight_eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of the form [L_1, L_2, …, L_n], where each L_i is a list of
instance weights on the i-th validation set.</p></li>
<li><p><strong>eval_metric</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>list of str</em><em>, or </em><em>callable</em><em>, </em><em>optional</em>) – If a str, should be a built-in evaluation metric to use. See
doc/parameter.rst.
If a list of str, should be the list of multiple built-in evaluation metrics
to use.
If callable, a custom evaluation metric. The call
signature is <code class="docutils literal notranslate"><span class="pre">func(y_predicted,</span> <span class="pre">y_true)</span></code> where <code class="docutils literal notranslate"><span class="pre">y_true</span></code> will be a
DMatrix object such that you may need to call the <code class="docutils literal notranslate"><span class="pre">get_label</span></code>
method. It must return a str, value pair where the str is a name
for the evaluation and value is the value of the evaluation
function. The callable custom objective is always minimized.</p></li>
<li><p><strong>early_stopping_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Activates early stopping. Validation metric needs to improve at least once in
every <strong>early_stopping_rounds</strong> round(s) to continue training.
Requires at least one item in <strong>eval_set</strong>.
The method returns the model from the last iteration (not the best one).
If there’s more than one item in <strong>eval_set</strong>, the last entry will be used
for early stopping.
If there’s more than one metric in <strong>eval_metric</strong>, the last metric will be
used for early stopping.
If early stopping occurs, the model will have three additional fields:
<code class="docutils literal notranslate"><span class="pre">clf.best_score</span></code>, <code class="docutils literal notranslate"><span class="pre">clf.best_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">clf.best_ntree_limit</span></code>.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <cite>verbose</cite> and an evaluation set is used, writes the evaluation
metric measured on the validation set to stderr.</p></li>
<li><p><strong>xgb_model</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be
loaded before training (allows training continuation).</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback functions</em>) – <p>List of callback functions that are applied at end of each iteration.
It is possible to use predefined callbacks by using <a class="reference internal" href="python_api.html#callback-api"><span class="std std-ref">Callback API</span></a>.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">reset_learning_rate</span><span class="p">(</span><span class="n">custom_rates</span><span class="p">)]</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.get_booster">
<code class="sig-name descname">get_booster</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.get_booster" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the underlying xgboost Booster of this model.</p>
<p>This will raise an exception when fit was not called</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>booster</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>a xgboost booster of underlying model</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.get_num_boosting_rounds">
<code class="sig-name descname">get_num_boosting_rounds</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.get_num_boosting_rounds" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the number of xgboost boosting rounds.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=False</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.get_xgb_params">
<code class="sig-name descname">get_xgb_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.get_xgb_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get xgboost type parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.intercept_">
<em class="property">property </em><code class="sig-name descname">intercept_</code><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (bias) property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Intercept is defined only for linear learners</p>
<p>Intercept (bias) is only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>intercept_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the model from a file.</p>
<p>The model is loaded from an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em><em> or </em><em>a memory buffer</em>) – Input file name or memory buffer(see also save_raw)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">output_margin=False</em>, <em class="sig-param">ntree_limit=None</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict with <cite>data</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe.</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">xgb.copy()</span></code> to make copies
of model object and then call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">predict()</span></code> with DART booster</p>
<p>If the booster object is DART type, <code class="docutils literal notranslate"><span class="pre">predict()</span></code> will perform dropouts, i.e. only
some of the trees will be evaluated. This will produce incorrect results if <code class="docutils literal notranslate"><span class="pre">data</span></code> is
not the training data. To obtain correct results on test sets, set <code class="docutils literal notranslate"><span class="pre">ntree_limit</span></code> to
a nonzero value, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">bst</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">,</span> <span class="n">ntree_limit</span><span class="o">=</span><span class="n">num_round</span><span class="p">)</span>
</pre></div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – The dmatrix storing the input.</p></li>
<li><p><strong>output_margin</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to output the raw untransformed margin value.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to best_ntree_limit if defined
(i.e. it has been trained with early stopping), otherwise 0 (use all trees).</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">ntree_limit=None</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the probability of each <cite>data</cite> example being of a given class.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">xgb.copy()</span></code> to make copies
of model object and then call predict</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – The dmatrix storing the input.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to best_ntree_limit if defined
(i.e. it has been trained with early stopping), otherwise 0 (use all trees).</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong> – a numpy array with the probability of each data example being of a given class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to a file.</p>
<p>The model is saved in an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em>) – Output file name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBClassifier.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
Modification of the sklearn method to allow unknown kwargs. This allows using
the full range of xgboost parameters that are not defined as member variables
in sklearn grid search.
:returns:
:rtype: self</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="xgboost.XGBRanker">
<em class="property">class </em><code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">XGBRanker</code><span class="sig-paren">(</span><em class="sig-param">max_depth=3</em>, <em class="sig-param">learning_rate=0.1</em>, <em class="sig-param">n_estimators=100</em>, <em class="sig-param">verbosity=1</em>, <em class="sig-param">silent=None</em>, <em class="sig-param">objective='rank:pairwise'</em>, <em class="sig-param">booster='gbtree'</em>, <em class="sig-param">n_jobs=-1</em>, <em class="sig-param">nthread=None</em>, <em class="sig-param">gamma=0</em>, <em class="sig-param">min_child_weight=1</em>, <em class="sig-param">max_delta_step=0</em>, <em class="sig-param">subsample=1</em>, <em class="sig-param">colsample_bytree=1</em>, <em class="sig-param">colsample_bylevel=1</em>, <em class="sig-param">colsample_bynode=1</em>, <em class="sig-param">reg_alpha=0</em>, <em class="sig-param">reg_lambda=1</em>, <em class="sig-param">scale_pos_weight=1</em>, <em class="sig-param">base_score=0.5</em>, <em class="sig-param">random_state=0</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">missing=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xgboost.sklearn.XGBModel</span></code></p>
<p>Implementation of the Scikit-Learn API for XGBoost Ranking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_depth</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum tree depth for base learners.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Boosting learning rate (xgb’s “eta”)</p></li>
<li><p><strong>n_estimators</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of boosted trees to fit.</p></li>
<li><p><strong>verbosity</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).</p></li>
<li><p><strong>silent</strong> (<em>boolean</em>) – Whether to print messages while running boosting. Deprecated. Use verbosity instead.</p></li>
<li><p><strong>objective</strong> (<em>string</em>) – Specify the learning task and the corresponding learning objective.
The objective name must start with “rank:”.</p></li>
<li><p><strong>booster</strong> (<em>string</em>) – Specify which booster to use: gbtree, gblinear or dart.</p></li>
<li><p><strong>nthread</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (Deprecated, please use <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>)</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (replaces <code class="docutils literal notranslate"><span class="pre">nthread</span></code>)</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Minimum loss reduction required to make a further partition on a leaf node of the tree.</p></li>
<li><p><strong>min_child_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Minimum sum of instance weight(hessian) needed in a child.</p></li>
<li><p><strong>max_delta_step</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum delta step we allow each tree’s weight estimation to be.</p></li>
<li><p><strong>subsample</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of the training instance.</p></li>
<li><p><strong>colsample_bytree</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns when constructing each tree.</p></li>
<li><p><strong>colsample_bylevel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each level.</p></li>
<li><p><strong>colsample_bynode</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each split.</p></li>
<li><p><strong>reg_alpha</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's alpha</em><em>)</em>) – L1 regularization term on weights</p></li>
<li><p><strong>reg_lambda</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's lambda</em><em>)</em>) – L2 regularization term on weights</p></li>
<li><p><strong>scale_pos_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Balancing of positive and negative weights.</p></li>
<li><p><strong>base_score</strong> – The initial prediction score of all instances, global bias.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (Deprecated, please use random_state)</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (replaces seed)</p></li>
<li><p><strong>missing</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Value in the data which needs to be present as a missing value. If
None, defaults to np.nan.</p></li>
<li><p><strong>**kwargs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>Keyword arguments for XGBoost Booster object.  Full documentation of parameters can
be found here: <a class="reference external" href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst">https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst</a>.
Attempting to set a parameter via the constructor args and **kwargs dict
simultaneously will result in a TypeError.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>**kwargs unsupported by scikit-learn</p>
<p>**kwargs is unsupported by scikit-learn.  We do not guarantee that parameters
passed via this argument will interact properly with scikit-learn.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A custom objective function is currently not supported by XGBRanker.
Likewise, a custom metric function is not supported either.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Query group information is required for ranking tasks.</p>
<p>Before fitting the model, your data need to be sorted by query group. When
fitting the model, you need to provide an additional array that
contains the size of each query group.</p>
<p>For example, if your original data look like:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 33%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>qid</p></td>
<td><p>label</p></td>
<td><p>features</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>x_1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>x_2</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>x_3</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>0</p></td>
<td><p>x_4</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>x_5</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>x_6</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>x_7</p></td>
</tr>
</tbody>
</table>
<p>then your group array should be <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">4]</span></code>.</p>
</div>
<dl class="method">
<dt id="xgboost.XGBRanker.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">ntree_limit=0</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the predicted leaf every tree for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em><em>, </em><em>shape=</em><em>[</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Input features matrix.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to 0 (use all trees).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X and for each tree, return the index of the
leaf x ends up in. Leaves are numbered within
<code class="docutils literal notranslate"><span class="pre">[0;</span> <span class="pre">2**(self.max_depth+1))</span></code>, possibly with gaps in the numbering.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array_like, shape=[n_samples, n_trees]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.coef_">
<em class="property">property </em><code class="sig-name descname">coef_</code><a class="headerlink" href="python_api.html#xgboost.XGBRanker.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Coefficients are defined only for linear learners</p>
<p>Coefficients are only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>coef_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes,</span> <span class="pre">n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.evals_result">
<code class="sig-name descname">evals_result</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.evals_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the evaluation results.</p>
<p>If <strong>eval_set</strong> is passed to the <cite>fit</cite> function, you can call
<code class="docutils literal notranslate"><span class="pre">evals_result()</span></code> to get evaluation results for all passed <strong>eval_sets</strong>.
When <strong>eval_metric</strong> is also passed to the <cite>fit</cite> function, the
<strong>evals_result</strong> will contain the <strong>eval_metrics</strong> passed to the <cite>fit</cite> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>evals_result</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dictionary</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBModel</span><span class="p">(</span><span class="o">**</span><span class="n">param_dist</span><span class="p">)</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)],</span>
        <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">evals_result</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">evals_result</span><span class="p">()</span>
</pre></div>
</div>
<p>The variable <strong>evals_result</strong> will contain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;validation_0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.604835&#39;</span><span class="p">,</span> <span class="s1">&#39;0.531479&#39;</span><span class="p">]},</span>
<span class="s1">&#39;validation_1&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.41965&#39;</span><span class="p">,</span> <span class="s1">&#39;0.17686&#39;</span><span class="p">]}}</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.feature_importances_">
<em class="property">property </em><code class="sig-name descname">feature_importances_</code><a class="headerlink" href="python_api.html#xgboost.XGBRanker.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature importances property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature importance is defined only for tree boosters</p>
<p>Feature importance is only defined when the decision tree model is chosen as base
learner (<cite>booster=gbtree</cite>). It is not defined for other base learner types, such
as linear learners (<cite>booster=gblinear</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_importances_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">group</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">sample_weight_eval_set=None</em>, <em class="sig-param">eval_group=None</em>, <em class="sig-param">eval_metric=None</em>, <em class="sig-param">early_stopping_rounds=None</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">xgb_model=None</em>, <em class="sig-param">callbacks=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit gradient boosting ranker</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em>) – Feature matrix</p></li>
<li><p><strong>y</strong> (<em>array_like</em>) – Labels</p></li>
<li><p><strong>group</strong> (<em>array_like</em>) – Size of each query group of training data. Should have as many elements as
the query groups in the training data</p></li>
<li><p><strong>sample_weight</strong> (<em>array_like</em>) – <p>Query group weights</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Weights are per-group for ranking tasks</p>
<p>In ranking task, one weight is assigned to each query group (not each
data point). This is because we only care about the relative ordering of
data points within each group, so it doesn’t make sense to assign
weights to individual data points.</p>
</div>
</p></li>
<li><p><strong>eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of (X, y) tuple pairs to use as validation sets, for which
metrics will be computed.
Validation metrics will help us track the performance of the model.</p></li>
<li><p><strong>sample_weight_eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – <p>A list of the form [L_1, L_2, …, L_n], where each L_i is a list of
group weights on the i-th validation set.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Weights are per-group for ranking tasks</p>
<p>In ranking task, one weight is assigned to each query group (not each
data point). This is because we only care about the relative ordering of
data points within each group, so it doesn’t make sense to assign
weights to individual data points.</p>
</div>
</p></li>
<li><p><strong>eval_group</strong> (<em>list of arrays</em><em>, </em><em>optional</em>) – A list in which <code class="docutils literal notranslate"><span class="pre">eval_group[i]</span></code> is the list containing the sizes of all
query groups in the <code class="docutils literal notranslate"><span class="pre">i</span></code>-th pair in <strong>eval_set</strong>.</p></li>
<li><p><strong>eval_metric</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>list of str</em><em>, </em><em>optional</em>) – If a str, should be a built-in evaluation metric to use. See
doc/parameter.rst.
If a list of str, should be the list of multiple built-in evaluation metrics
to use. The custom evaluation metric is not yet supported for the ranker.</p></li>
<li><p><strong>early_stopping_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Activates early stopping. Validation metric needs to improve at least once in
every <strong>early_stopping_rounds</strong> round(s) to continue training.
Requires at least one item in <strong>eval_set</strong>.
The method returns the model from the last iteration (not the best one).
If there’s more than one item in <strong>eval_set</strong>, the last entry will be used
for early stopping.
If there’s more than one metric in <strong>eval_metric</strong>, the last metric will be
used for early stopping.
If early stopping occurs, the model will have three additional fields:
<code class="docutils literal notranslate"><span class="pre">clf.best_score</span></code>, <code class="docutils literal notranslate"><span class="pre">clf.best_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">clf.best_ntree_limit</span></code>.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <cite>verbose</cite> and an evaluation set is used, writes the evaluation
metric measured on the validation set to stderr.</p></li>
<li><p><strong>xgb_model</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be
loaded before training (allows training continuation).</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback functions</em>) – <p>List of callback functions that are applied at end of each iteration.
It is possible to use predefined callbacks by using <a class="reference internal" href="python_api.html#callback-api"><span class="std std-ref">Callback API</span></a>.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">reset_learning_rate</span><span class="p">(</span><span class="n">custom_rates</span><span class="p">)]</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.get_booster">
<code class="sig-name descname">get_booster</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.get_booster" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the underlying xgboost Booster of this model.</p>
<p>This will raise an exception when fit was not called</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>booster</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>a xgboost booster of underlying model</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.get_num_boosting_rounds">
<code class="sig-name descname">get_num_boosting_rounds</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.get_num_boosting_rounds" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the number of xgboost boosting rounds.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=False</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.get_xgb_params">
<code class="sig-name descname">get_xgb_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.get_xgb_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get xgboost type parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.intercept_">
<em class="property">property </em><code class="sig-name descname">intercept_</code><a class="headerlink" href="python_api.html#xgboost.XGBRanker.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (bias) property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Intercept is defined only for linear learners</p>
<p>Intercept (bias) is only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>intercept_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the model from a file.</p>
<p>The model is loaded from an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em><em> or </em><em>a memory buffer</em>) – Input file name or memory buffer(see also save_raw)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">output_margin=False</em>, <em class="sig-param">ntree_limit=0</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict with <cite>data</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe.</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">xgb.copy()</span></code> to make copies
of model object and then call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">predict()</span></code> with DART booster</p>
<p>If the booster object is DART type, <code class="docutils literal notranslate"><span class="pre">predict()</span></code> will perform dropouts, i.e. only
some of the trees will be evaluated. This will produce incorrect results if <code class="docutils literal notranslate"><span class="pre">data</span></code> is
not the training data. To obtain correct results on test sets, set <code class="docutils literal notranslate"><span class="pre">ntree_limit</span></code> to
a nonzero value, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">bst</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">,</span> <span class="n">ntree_limit</span><span class="o">=</span><span class="n">num_round</span><span class="p">)</span>
</pre></div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>numpy.array/scipy.sparse</em>) – Data to predict with</p></li>
<li><p><strong>output_margin</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to output the raw untransformed margin value.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to best_ntree_limit if defined
(i.e. it has been trained with early stopping), otherwise 0 (use all trees).</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to a file.</p>
<p>The model is saved in an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em>) – Output file name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRanker.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRanker.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
Modification of the sklearn method to allow unknown kwargs. This allows using
the full range of xgboost parameters that are not defined as member variables
in sklearn grid search.
:returns:
:rtype: self</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="xgboost.XGBRFRegressor">
<em class="property">class </em><code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">XGBRFRegressor</code><span class="sig-paren">(</span><em class="sig-param">max_depth=3</em>, <em class="sig-param">learning_rate=1</em>, <em class="sig-param">n_estimators=100</em>, <em class="sig-param">verbosity=1</em>, <em class="sig-param">silent=None</em>, <em class="sig-param">objective='reg:squarederror'</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">nthread=None</em>, <em class="sig-param">gamma=0</em>, <em class="sig-param">min_child_weight=1</em>, <em class="sig-param">max_delta_step=0</em>, <em class="sig-param">subsample=0.8</em>, <em class="sig-param">colsample_bytree=1</em>, <em class="sig-param">colsample_bylevel=1</em>, <em class="sig-param">colsample_bynode=0.8</em>, <em class="sig-param">reg_alpha=0</em>, <em class="sig-param">reg_lambda=1e-05</em>, <em class="sig-param">scale_pos_weight=1</em>, <em class="sig-param">base_score=0.5</em>, <em class="sig-param">random_state=0</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">missing=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xgboost.sklearn.XGBRegressor</span></code></p>
<p>Experimental implementation of the scikit-learn API for XGBoost random forest regression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_depth</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum tree depth for base learners.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Boosting learning rate (xgb’s “eta”)</p></li>
<li><p><strong>n_estimators</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of trees to fit.</p></li>
<li><p><strong>verbosity</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).</p></li>
<li><p><strong>silent</strong> (<em>boolean</em>) – Whether to print messages while running boosting. Deprecated. Use verbosity instead.</p></li>
<li><p><strong>objective</strong> (<em>string</em><em> or </em><em>callable</em>) – Specify the learning task and the corresponding learning objective or
a custom objective function to be used (see note below).</p></li>
<li><p><strong>booster</strong> (<em>string</em>) – Specify which booster to use: gbtree, gblinear or dart.</p></li>
<li><p><strong>nthread</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (Deprecated, please use <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>)</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (replaces <code class="docutils literal notranslate"><span class="pre">nthread</span></code>)</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Minimum loss reduction required to make a further partition on a leaf node of the tree.</p></li>
<li><p><strong>min_child_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Minimum sum of instance weight(hessian) needed in a child.</p></li>
<li><p><strong>max_delta_step</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum delta step we allow each tree’s weight estimation to be.</p></li>
<li><p><strong>subsample</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of the training instance.</p></li>
<li><p><strong>colsample_bytree</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns when constructing each tree.</p></li>
<li><p><strong>colsample_bylevel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each level.</p></li>
<li><p><strong>colsample_bynode</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each split.</p></li>
<li><p><strong>reg_alpha</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's alpha</em><em>)</em>) – L1 regularization term on weights</p></li>
<li><p><strong>reg_lambda</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's lambda</em><em>)</em>) – L2 regularization term on weights</p></li>
<li><p><strong>scale_pos_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Balancing of positive and negative weights.</p></li>
<li><p><strong>base_score</strong> – The initial prediction score of all instances, global bias.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (Deprecated, please use random_state)</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (replaces seed)</p></li>
<li><p><strong>missing</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Value in the data which needs to be present as a missing value. If
None, defaults to np.nan.</p></li>
<li><p><strong>importance_type</strong> (<em>string</em><em>, </em><em>default &quot;gain&quot;</em>) – The feature importance type for the feature_importances_ property:
either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.</p></li>
<li><p><strong>**kwargs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>Keyword arguments for XGBoost Booster object.  Full documentation of parameters can
be found here: <a class="reference external" href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst">https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst</a>.
Attempting to set a parameter via the constructor args and **kwargs dict simultaneously
will result in a TypeError.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>**kwargs unsupported by scikit-learn</p>
<p>**kwargs is unsupported by scikit-learn.  We do not guarantee that parameters
passed via this argument will interact properly with scikit-learn.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A custom objective function can be provided for the <code class="docutils literal notranslate"><span class="pre">objective</span></code>
parameter. In this case, it should have the signature
<code class="docutils literal notranslate"><span class="pre">objective(y_true,</span> <span class="pre">y_pred)</span> <span class="pre">-&gt;</span> <span class="pre">grad,</span> <span class="pre">hess</span></code>:</p>
<dl class="simple">
<dt>y_true: array_like of shape [n_samples]</dt><dd><p>The target values</p>
</dd>
<dt>y_pred: array_like of shape [n_samples]</dt><dd><p>The predicted values</p>
</dd>
<dt>grad: array_like of shape [n_samples]</dt><dd><p>The value of the gradient for each sample point.</p>
</dd>
<dt>hess: array_like of shape [n_samples]</dt><dd><p>The value of the second derivative for each sample point</p>
</dd>
</dl>
</div>
<dl class="method">
<dt id="xgboost.XGBRFRegressor.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">ntree_limit=0</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the predicted leaf every tree for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em><em>, </em><em>shape=</em><em>[</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Input features matrix.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to 0 (use all trees).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X and for each tree, return the index of the
leaf x ends up in. Leaves are numbered within
<code class="docutils literal notranslate"><span class="pre">[0;</span> <span class="pre">2**(self.max_depth+1))</span></code>, possibly with gaps in the numbering.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array_like, shape=[n_samples, n_trees]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.coef_">
<em class="property">property </em><code class="sig-name descname">coef_</code><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Coefficients are defined only for linear learners</p>
<p>Coefficients are only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>coef_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes,</span> <span class="pre">n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.evals_result">
<code class="sig-name descname">evals_result</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.evals_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the evaluation results.</p>
<p>If <strong>eval_set</strong> is passed to the <cite>fit</cite> function, you can call
<code class="docutils literal notranslate"><span class="pre">evals_result()</span></code> to get evaluation results for all passed <strong>eval_sets</strong>.
When <strong>eval_metric</strong> is also passed to the <cite>fit</cite> function, the
<strong>evals_result</strong> will contain the <strong>eval_metrics</strong> passed to the <cite>fit</cite> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>evals_result</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dictionary</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBModel</span><span class="p">(</span><span class="o">**</span><span class="n">param_dist</span><span class="p">)</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)],</span>
        <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">evals_result</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">evals_result</span><span class="p">()</span>
</pre></div>
</div>
<p>The variable <strong>evals_result</strong> will contain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;validation_0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.604835&#39;</span><span class="p">,</span> <span class="s1">&#39;0.531479&#39;</span><span class="p">]},</span>
<span class="s1">&#39;validation_1&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.41965&#39;</span><span class="p">,</span> <span class="s1">&#39;0.17686&#39;</span><span class="p">]}}</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.feature_importances_">
<em class="property">property </em><code class="sig-name descname">feature_importances_</code><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature importances property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature importance is defined only for tree boosters</p>
<p>Feature importance is only defined when the decision tree model is chosen as base
learner (<cite>booster=gbtree</cite>). It is not defined for other base learner types, such
as linear learners (<cite>booster=gblinear</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_importances_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">eval_metric=None</em>, <em class="sig-param">early_stopping_rounds=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">xgb_model=None</em>, <em class="sig-param">sample_weight_eval_set=None</em>, <em class="sig-param">callbacks=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit gradient boosting model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em>) – Feature matrix</p></li>
<li><p><strong>y</strong> (<em>array_like</em>) – Labels</p></li>
<li><p><strong>sample_weight</strong> (<em>array_like</em>) – instance weights</p></li>
<li><p><strong>eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of (X, y) tuple pairs to use as validation sets, for which
metrics will be computed.
Validation metrics will help us track the performance of the model.</p></li>
<li><p><strong>sample_weight_eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of the form [L_1, L_2, …, L_n], where each L_i is a list of
instance weights on the i-th validation set.</p></li>
<li><p><strong>eval_metric</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>list of str</em><em>, or </em><em>callable</em><em>, </em><em>optional</em>) – If a str, should be a built-in evaluation metric to use. See
doc/parameter.rst.
If a list of str, should be the list of multiple built-in evaluation metrics
to use.
If callable, a custom evaluation metric. The call
signature is <code class="docutils literal notranslate"><span class="pre">func(y_predicted,</span> <span class="pre">y_true)</span></code> where <code class="docutils literal notranslate"><span class="pre">y_true</span></code> will be a
DMatrix object such that you may need to call the <code class="docutils literal notranslate"><span class="pre">get_label</span></code>
method. It must return a str, value pair where the str is a name
for the evaluation and value is the value of the evaluation
function. The callable custom objective is always minimized.</p></li>
<li><p><strong>early_stopping_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Activates early stopping. Validation metric needs to improve at least once in
every <strong>early_stopping_rounds</strong> round(s) to continue training.
Requires at least one item in <strong>eval_set</strong>.
The method returns the model from the last iteration (not the best one).
If there’s more than one item in <strong>eval_set</strong>, the last entry will be used
for early stopping.
If there’s more than one metric in <strong>eval_metric</strong>, the last metric will be
used for early stopping.
If early stopping occurs, the model will have three additional fields:
<code class="docutils literal notranslate"><span class="pre">clf.best_score</span></code>, <code class="docutils literal notranslate"><span class="pre">clf.best_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">clf.best_ntree_limit</span></code>.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <cite>verbose</cite> and an evaluation set is used, writes the evaluation
metric measured on the validation set to stderr.</p></li>
<li><p><strong>xgb_model</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be
loaded before training (allows training continuation).</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback functions</em>) – <p>List of callback functions that are applied at end of each iteration.
It is possible to use predefined callbacks by using <a class="reference internal" href="python_api.html#callback-api"><span class="std std-ref">Callback API</span></a>.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">reset_learning_rate</span><span class="p">(</span><span class="n">custom_rates</span><span class="p">)]</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.get_booster">
<code class="sig-name descname">get_booster</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.get_booster" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the underlying xgboost Booster of this model.</p>
<p>This will raise an exception when fit was not called</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>booster</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>a xgboost booster of underlying model</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.get_num_boosting_rounds">
<code class="sig-name descname">get_num_boosting_rounds</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.get_num_boosting_rounds" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the number of xgboost boosting rounds.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=False</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.get_xgb_params">
<code class="sig-name descname">get_xgb_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.get_xgb_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get xgboost type parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.intercept_">
<em class="property">property </em><code class="sig-name descname">intercept_</code><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (bias) property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Intercept is defined only for linear learners</p>
<p>Intercept (bias) is only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>intercept_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the model from a file.</p>
<p>The model is loaded from an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em><em> or </em><em>a memory buffer</em>) – Input file name or memory buffer(see also save_raw)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">output_margin=False</em>, <em class="sig-param">ntree_limit=None</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict with <cite>data</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe.</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">xgb.copy()</span></code> to make copies
of model object and then call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">predict()</span></code> with DART booster</p>
<p>If the booster object is DART type, <code class="docutils literal notranslate"><span class="pre">predict()</span></code> will perform dropouts, i.e. only
some of the trees will be evaluated. This will produce incorrect results if <code class="docutils literal notranslate"><span class="pre">data</span></code> is
not the training data. To obtain correct results on test sets, set <code class="docutils literal notranslate"><span class="pre">ntree_limit</span></code> to
a nonzero value, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">bst</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">,</span> <span class="n">ntree_limit</span><span class="o">=</span><span class="n">num_round</span><span class="p">)</span>
</pre></div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>numpy.array/scipy.sparse</em>) – Data to predict with</p></li>
<li><p><strong>output_margin</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to output the raw untransformed margin value.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to best_ntree_limit if defined
(i.e. it has been trained with early stopping), otherwise 0 (use all trees).</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to a file.</p>
<p>The model is saved in an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em>) – Output file name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFRegressor.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFRegressor.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
Modification of the sklearn method to allow unknown kwargs. This allows using
the full range of xgboost parameters that are not defined as member variables
in sklearn grid search.
:returns:
:rtype: self</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="xgboost.XGBRFClassifier">
<em class="property">class </em><code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">XGBRFClassifier</code><span class="sig-paren">(</span><em class="sig-param">max_depth=3</em>, <em class="sig-param">learning_rate=1</em>, <em class="sig-param">n_estimators=100</em>, <em class="sig-param">verbosity=1</em>, <em class="sig-param">silent=None</em>, <em class="sig-param">objective='binary:logistic'</em>, <em class="sig-param">n_jobs=1</em>, <em class="sig-param">nthread=None</em>, <em class="sig-param">gamma=0</em>, <em class="sig-param">min_child_weight=1</em>, <em class="sig-param">max_delta_step=0</em>, <em class="sig-param">subsample=0.8</em>, <em class="sig-param">colsample_bytree=1</em>, <em class="sig-param">colsample_bylevel=1</em>, <em class="sig-param">colsample_bynode=0.8</em>, <em class="sig-param">reg_alpha=0</em>, <em class="sig-param">reg_lambda=1e-05</em>, <em class="sig-param">scale_pos_weight=1</em>, <em class="sig-param">base_score=0.5</em>, <em class="sig-param">random_state=0</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">missing=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xgboost.sklearn.XGBClassifier</span></code></p>
<p>Experimental implementation of the scikit-learn API for XGBoost random forest classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_depth</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum tree depth for base learners.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Boosting learning rate (xgb’s “eta”)</p></li>
<li><p><strong>n_estimators</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of trees to fit.</p></li>
<li><p><strong>verbosity</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).</p></li>
<li><p><strong>silent</strong> (<em>boolean</em>) – Whether to print messages while running boosting. Deprecated. Use verbosity instead.</p></li>
<li><p><strong>objective</strong> (<em>string</em><em> or </em><em>callable</em>) – Specify the learning task and the corresponding learning objective or
a custom objective function to be used (see note below).</p></li>
<li><p><strong>booster</strong> (<em>string</em>) – Specify which booster to use: gbtree, gblinear or dart.</p></li>
<li><p><strong>nthread</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (Deprecated, please use <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>)</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of parallel threads used to run xgboost.  (replaces <code class="docutils literal notranslate"><span class="pre">nthread</span></code>)</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Minimum loss reduction required to make a further partition on a leaf node of the tree.</p></li>
<li><p><strong>min_child_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Minimum sum of instance weight(hessian) needed in a child.</p></li>
<li><p><strong>max_delta_step</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Maximum delta step we allow each tree’s weight estimation to be.</p></li>
<li><p><strong>subsample</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of the training instance.</p></li>
<li><p><strong>colsample_bytree</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns when constructing each tree.</p></li>
<li><p><strong>colsample_bylevel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each level.</p></li>
<li><p><strong>colsample_bynode</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Subsample ratio of columns for each split.</p></li>
<li><p><strong>reg_alpha</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's alpha</em><em>)</em>) – L1 regularization term on weights</p></li>
<li><p><strong>reg_lambda</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> (</em><em>xgb's lambda</em><em>)</em>) – L2 regularization term on weights</p></li>
<li><p><strong>scale_pos_weight</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Balancing of positive and negative weights.</p></li>
<li><p><strong>base_score</strong> – The initial prediction score of all instances, global bias.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (Deprecated, please use random_state)</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random number seed.  (replaces seed)</p></li>
<li><p><strong>missing</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Value in the data which needs to be present as a missing value. If
None, defaults to np.nan.</p></li>
<li><p><strong>importance_type</strong> (<em>string</em><em>, </em><em>default &quot;gain&quot;</em>) – The feature importance type for the feature_importances_ property:
either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.</p></li>
<li><p><strong>**kwargs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>Keyword arguments for XGBoost Booster object.  Full documentation of parameters can
be found here: <a class="reference external" href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst">https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst</a>.
Attempting to set a parameter via the constructor args and **kwargs dict simultaneously
will result in a TypeError.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>**kwargs unsupported by scikit-learn</p>
<p>**kwargs is unsupported by scikit-learn.  We do not guarantee that parameters
passed via this argument will interact properly with scikit-learn.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A custom objective function can be provided for the <code class="docutils literal notranslate"><span class="pre">objective</span></code>
parameter. In this case, it should have the signature
<code class="docutils literal notranslate"><span class="pre">objective(y_true,</span> <span class="pre">y_pred)</span> <span class="pre">-&gt;</span> <span class="pre">grad,</span> <span class="pre">hess</span></code>:</p>
<dl class="simple">
<dt>y_true: array_like of shape [n_samples]</dt><dd><p>The target values</p>
</dd>
<dt>y_pred: array_like of shape [n_samples]</dt><dd><p>The predicted values</p>
</dd>
<dt>grad: array_like of shape [n_samples]</dt><dd><p>The value of the gradient for each sample point.</p>
</dd>
<dt>hess: array_like of shape [n_samples]</dt><dd><p>The value of the second derivative for each sample point</p>
</dd>
</dl>
</div>
<dl class="method">
<dt id="xgboost.XGBRFClassifier.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">ntree_limit=0</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the predicted leaf every tree for each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em><em>, </em><em>shape=</em><em>[</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Input features matrix.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to 0 (use all trees).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X and for each tree, return the index of the
leaf x ends up in. Leaves are numbered within
<code class="docutils literal notranslate"><span class="pre">[0;</span> <span class="pre">2**(self.max_depth+1))</span></code>, possibly with gaps in the numbering.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array_like, shape=[n_samples, n_trees]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.coef_">
<em class="property">property </em><code class="sig-name descname">coef_</code><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Coefficients are defined only for linear learners</p>
<p>Coefficients are only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>coef_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes,</span> <span class="pre">n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.evals_result">
<code class="sig-name descname">evals_result</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.evals_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the evaluation results.</p>
<p>If <strong>eval_set</strong> is passed to the <cite>fit</cite> function, you can call
<code class="docutils literal notranslate"><span class="pre">evals_result()</span></code> to get evaluation results for all passed <strong>eval_sets</strong>.
When <strong>eval_metric</strong> is also passed to the <cite>fit</cite> function, the
<strong>evals_result</strong> will contain the <strong>eval_metrics</strong> passed to the <cite>fit</cite> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>evals_result</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dictionary</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">param_dist</span><span class="p">)</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)],</span>
        <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">evals_result</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">evals_result</span><span class="p">()</span>
</pre></div>
</div>
<p>The variable <strong>evals_result</strong> will contain</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;validation_0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.604835&#39;</span><span class="p">,</span> <span class="s1">&#39;0.531479&#39;</span><span class="p">]},</span>
<span class="s1">&#39;validation_1&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;0.41965&#39;</span><span class="p">,</span> <span class="s1">&#39;0.17686&#39;</span><span class="p">]}}</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.feature_importances_">
<em class="property">property </em><code class="sig-name descname">feature_importances_</code><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Feature importances property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature importance is defined only for tree boosters</p>
<p>Feature importance is only defined when the decision tree model is chosen as base
learner (<cite>booster=gbtree</cite>). It is not defined for other base learner types, such
as linear learners (<cite>booster=gblinear</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_importances_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">[n_features]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">eval_set=None</em>, <em class="sig-param">eval_metric=None</em>, <em class="sig-param">early_stopping_rounds=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">xgb_model=None</em>, <em class="sig-param">sample_weight_eval_set=None</em>, <em class="sig-param">callbacks=None</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit gradient boosting classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array_like</em>) – Feature matrix</p></li>
<li><p><strong>y</strong> (<em>array_like</em>) – Labels</p></li>
<li><p><strong>sample_weight</strong> (<em>array_like</em>) – instance weights</p></li>
<li><p><strong>eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of (X, y) tuple pairs to use as validation sets, for which
metrics will be computed.
Validation metrics will help us track the performance of the model.</p></li>
<li><p><strong>sample_weight_eval_set</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – A list of the form [L_1, L_2, …, L_n], where each L_i is a list of
instance weights on the i-th validation set.</p></li>
<li><p><strong>eval_metric</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>list of str</em><em>, or </em><em>callable</em><em>, </em><em>optional</em>) – If a str, should be a built-in evaluation metric to use. See
doc/parameter.rst.
If a list of str, should be the list of multiple built-in evaluation metrics
to use.
If callable, a custom evaluation metric. The call
signature is <code class="docutils literal notranslate"><span class="pre">func(y_predicted,</span> <span class="pre">y_true)</span></code> where <code class="docutils literal notranslate"><span class="pre">y_true</span></code> will be a
DMatrix object such that you may need to call the <code class="docutils literal notranslate"><span class="pre">get_label</span></code>
method. It must return a str, value pair where the str is a name
for the evaluation and value is the value of the evaluation
function. The callable custom objective is always minimized.</p></li>
<li><p><strong>early_stopping_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Activates early stopping. Validation metric needs to improve at least once in
every <strong>early_stopping_rounds</strong> round(s) to continue training.
Requires at least one item in <strong>eval_set</strong>.
The method returns the model from the last iteration (not the best one).
If there’s more than one item in <strong>eval_set</strong>, the last entry will be used
for early stopping.
If there’s more than one metric in <strong>eval_metric</strong>, the last metric will be
used for early stopping.
If early stopping occurs, the model will have three additional fields:
<code class="docutils literal notranslate"><span class="pre">clf.best_score</span></code>, <code class="docutils literal notranslate"><span class="pre">clf.best_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">clf.best_ntree_limit</span></code>.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <cite>verbose</cite> and an evaluation set is used, writes the evaluation
metric measured on the validation set to stderr.</p></li>
<li><p><strong>xgb_model</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be
loaded before training (allows training continuation).</p></li>
<li><p><strong>callbacks</strong> (<em>list of callback functions</em>) – <p>List of callback functions that are applied at end of each iteration.
It is possible to use predefined callbacks by using <a class="reference internal" href="python_api.html#callback-api"><span class="std std-ref">Callback API</span></a>.
Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">xgb</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">reset_learning_rate</span><span class="p">(</span><span class="n">custom_rates</span><span class="p">)]</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.get_booster">
<code class="sig-name descname">get_booster</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.get_booster" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the underlying xgboost Booster of this model.</p>
<p>This will raise an exception when fit was not called</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>booster</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>a xgboost booster of underlying model</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.get_num_boosting_rounds">
<code class="sig-name descname">get_num_boosting_rounds</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.get_num_boosting_rounds" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the number of xgboost boosting rounds.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=False</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.get_xgb_params">
<code class="sig-name descname">get_xgb_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.get_xgb_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get xgboost type parameters.</p>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.intercept_">
<em class="property">property </em><code class="sig-name descname">intercept_</code><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (bias) property</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Intercept is defined only for linear learners</p>
<p>Intercept (bias) is only defined when the linear model is chosen as base
learner (<cite>booster=gblinear</cite>). It is not defined for other base learner types, such
as tree learners (<cite>booster=gbtree</cite>).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>intercept_</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>array of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code> or <code class="docutils literal notranslate"><span class="pre">[n_classes]</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the model from a file.</p>
<p>The model is loaded from an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em><em> or </em><em>a memory buffer</em>) – Input file name or memory buffer(see also save_raw)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">output_margin=False</em>, <em class="sig-param">ntree_limit=None</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict with <cite>data</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe.</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">xgb.copy()</span></code> to make copies
of model object and then call <code class="docutils literal notranslate"><span class="pre">predict()</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">predict()</span></code> with DART booster</p>
<p>If the booster object is DART type, <code class="docutils literal notranslate"><span class="pre">predict()</span></code> will perform dropouts, i.e. only
some of the trees will be evaluated. This will produce incorrect results if <code class="docutils literal notranslate"><span class="pre">data</span></code> is
not the training data. To obtain correct results on test sets, set <code class="docutils literal notranslate"><span class="pre">ntree_limit</span></code> to
a nonzero value, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">bst</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">,</span> <span class="n">ntree_limit</span><span class="o">=</span><span class="n">num_round</span><span class="p">)</span>
</pre></div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – The dmatrix storing the input.</p></li>
<li><p><strong>output_margin</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to output the raw untransformed margin value.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to best_ntree_limit if defined
(i.e. it has been trained with early stopping), otherwise 0 (use all trees).</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.predict_proba">
<code class="sig-name descname">predict_proba</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">ntree_limit=None</em>, <em class="sig-param">validate_features=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the probability of each <cite>data</cite> example being of a given class.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not thread safe</p>
<p>For each booster object, predict can only be called from one thread.
If you want to run prediction using multiple thread, call <code class="docutils literal notranslate"><span class="pre">xgb.copy()</span></code> to make copies
of model object and then call predict</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="python_api.html#xgboost.DMatrix" title="xgboost.DMatrix"><em>DMatrix</em></a>) – The dmatrix storing the input.</p></li>
<li><p><strong>ntree_limit</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Limit number of trees in the prediction; defaults to best_ntree_limit if defined
(i.e. it has been trained with early stopping), otherwise 0 (use all trees).</p></li>
<li><p><strong>validate_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When this is True, validate that the Booster’s and data’s feature_names are identical.
Otherwise, it is assumed that the feature_names are the same.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>prediction</strong> – a numpy array with the probability of each data example being of a given class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">fname</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to a file.</p>
<p>The model is saved in an XGBoost internal binary format which is
universal among the various XGBoost interfaces. Auxiliary attributes of
the Python Booster object (such as feature names) will not be loaded.
Label encodings (text labels to numeric labels) will be also lost.
<strong>If you are using only the Python interface, we recommend pickling the
model object for best results.</strong></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fname</strong> (<em>string</em>) – Output file name</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="xgboost.XGBRFClassifier.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.XGBRFClassifier.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
Modification of the sklearn method to allow unknown kwargs. This allows using
the full range of xgboost parameters that are not defined as member variables
in sklearn grid search.
:returns:
:rtype: self</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-xgboost.plotting">
<span id="plotting-api"></span><h2>Plotting API<a class="headerlink" href="python_api.html#module-xgboost.plotting" title="Permalink to this headline">¶</a></h2>
<p>Plotting Library.</p>
<dl class="function">
<dt id="xgboost.plot_importance">
<code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">plot_importance</code><span class="sig-paren">(</span><em class="sig-param">booster</em>, <em class="sig-param">ax=None</em>, <em class="sig-param">height=0.2</em>, <em class="sig-param">xlim=None</em>, <em class="sig-param">ylim=None</em>, <em class="sig-param">title='Feature importance'</em>, <em class="sig-param">xlabel='F score'</em>, <em class="sig-param">ylabel='Features'</em>, <em class="sig-param">importance_type='weight'</em>, <em class="sig-param">max_num_features=None</em>, <em class="sig-param">grid=True</em>, <em class="sig-param">show_values=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.plot_importance" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot importance based on fitted trees.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>booster</strong> (<a class="reference internal" href="python_api.html#xgboost.Booster" title="xgboost.Booster"><em>Booster</em></a><em>, </em><em>XGBModel</em><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Booster or XGBModel instance, or dict taken by Booster.get_fscore()</p></li>
<li><p><strong>ax</strong> (<em>matplotlib Axes</em><em>, </em><em>default None</em>) – Target axes instance. If None, new figure and axes will be created.</p></li>
<li><p><strong>grid</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>Turn the axes grids on</em><em> or </em><em>off.  Default is True</em><em> (</em><em>On</em><em>)</em><em></em>) – </p></li>
<li><p><strong>importance_type</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default &quot;weight&quot;</em>) – <p>How the importance is calculated: either “weight”, “gain”, or “cover”</p>
<ul>
<li><p>”weight” is the number of times a feature appears in a tree</p></li>
<li><p>”gain” is the average gain of splits which use the feature</p></li>
<li><p>”cover” is the average coverage of splits which use the feature
where coverage is defined as the number of samples affected by the split</p></li>
</ul>
</p></li>
<li><p><strong>max_num_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>default None</em>) – Maximum number of top features displayed on plot. If None, all features will be displayed.</p></li>
<li><p><strong>height</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>default 0.2</em>) – Bar height, passed to ax.barh()</p></li>
<li><p><strong>xlim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>, </em><em>default None</em>) – Tuple passed to axes.xlim()</p></li>
<li><p><strong>ylim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a><em>, </em><em>default None</em>) – Tuple passed to axes.ylim()</p></li>
<li><p><strong>title</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default &quot;Feature importance&quot;</em>) – Axes title. To disable, pass None.</p></li>
<li><p><strong>xlabel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default &quot;F score&quot;</em>) – X axis title label. To disable, pass None.</p></li>
<li><p><strong>ylabel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default &quot;Features&quot;</em>) – Y axis title label. To disable, pass None.</p></li>
<li><p><strong>show_values</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>default True</em>) – Show values on plot. To disable, pass False.</p></li>
<li><p><strong>kwargs</strong> – Other keywords passed to ax.barh()</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>ax</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>matplotlib Axes</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.plot_tree">
<code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">plot_tree</code><span class="sig-paren">(</span><em class="sig-param">booster</em>, <em class="sig-param">fmap=''</em>, <em class="sig-param">num_trees=0</em>, <em class="sig-param">rankdir=None</em>, <em class="sig-param">ax=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.plot_tree" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot specified tree.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>booster</strong> (<a class="reference internal" href="python_api.html#xgboost.Booster" title="xgboost.Booster"><em>Booster</em></a><em>, </em><em>XGBModel</em>) – Booster or XGBModel instance</p></li>
<li><p><strong>fmap</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em> (</em><em>optional</em><em>)</em>) – The name of feature map file</p></li>
<li><p><strong>num_trees</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>default 0</em>) – Specify the ordinal number of target tree</p></li>
<li><p><strong>rankdir</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default &quot;TB&quot;</em>) – Passed to graphiz via graph_attr</p></li>
<li><p><strong>ax</strong> (<em>matplotlib Axes</em><em>, </em><em>default None</em>) – Target axes instance. If None, new figure and axes will be created.</p></li>
<li><p><strong>kwargs</strong> – Other keywords passed to to_graphviz</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>ax</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>matplotlib Axes</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.to_graphviz">
<code class="sig-prename descclassname">xgboost.</code><code class="sig-name descname">to_graphviz</code><span class="sig-paren">(</span><em class="sig-param">booster</em>, <em class="sig-param">fmap=''</em>, <em class="sig-param">num_trees=0</em>, <em class="sig-param">rankdir=None</em>, <em class="sig-param">yes_color=None</em>, <em class="sig-param">no_color=None</em>, <em class="sig-param">condition_node_params=None</em>, <em class="sig-param">leaf_node_params=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.to_graphviz" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert specified tree to graphviz instance. IPython can automatically plot
the returned graphiz instance. Otherwise, you should call .render() method
of the returned graphiz instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>booster</strong> (<a class="reference internal" href="python_api.html#xgboost.Booster" title="xgboost.Booster"><em>Booster</em></a><em>, </em><em>XGBModel</em>) – Booster or XGBModel instance</p></li>
<li><p><strong>fmap</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em> (</em><em>optional</em><em>)</em>) – The name of feature map file</p></li>
<li><p><strong>num_trees</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>default 0</em>) – Specify the ordinal number of target tree</p></li>
<li><p><strong>rankdir</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default &quot;UT&quot;</em>) – Passed to graphiz via graph_attr</p></li>
<li><p><strong>yes_color</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default '#0000FF'</em>) – Edge color when meets the node condition.</p></li>
<li><p><strong>no_color</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>default '#FF0000'</em>) – Edge color when doesn’t meet the node condition.</p></li>
<li><p><strong>condition_node_params</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em> (</em><em>optional</em><em>)</em>) – <p>Condition node configuration for for graphviz.  Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl class="simple">
<dt>{‘shape’: ‘box’,</dt><dd><p>’style’: ‘filled,rounded’,
‘fillcolor’: ‘#78bceb’}</p>
</dd>
</dl>
</p></li>
<li><p><strong>leaf_node_params</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em> (</em><em>optional</em><em>)</em>) – <p>Leaf node configuration for graphviz. Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl class="simple">
<dt>{‘shape’: ‘box’,</dt><dd><p>’style’: ‘filled’,
‘fillcolor’: ‘#e48038’}</p>
</dd>
</dl>
</p></li>
<li><p><strong>kwargs</strong> (<em>Other keywords passed to graphviz graph_attr</em><em>, </em><em>E.g.:</em>) – <code class="docutils literal notranslate"><span class="pre">graph</span> <span class="pre">[</span> <span class="pre">{key}</span> <span class="pre">=</span> <span class="pre">{value}</span> <span class="pre">]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>graph</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>graphviz.Source</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="callback-api">
<span id="id1"></span><h2>Callback API<a class="headerlink" href="python_api.html#callback-api" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="xgboost.callback.print_evaluation">
<code class="sig-prename descclassname">xgboost.callback.</code><code class="sig-name descname">print_evaluation</code><span class="sig-paren">(</span><em class="sig-param">period=1</em>, <em class="sig-param">show_stdv=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.callback.print_evaluation" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a callback that print evaluation result.</p>
<p>We print the evaluation results every <strong>period</strong> iterations
and on the first and the last iterations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>period</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – The period to log the evaluation results</p></li>
<li><p><strong>show_stdv</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether show stdv if provided</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>callback</strong> – A callback that print evaluation every period iterations.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>function</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.callback.record_evaluation">
<code class="sig-prename descclassname">xgboost.callback.</code><code class="sig-name descname">record_evaluation</code><span class="sig-paren">(</span><em class="sig-param">eval_result</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.callback.record_evaluation" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a call back that records the evaluation history into <strong>eval_result</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_result</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – A dictionary to store the evaluation results.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>callback</strong> – The requested callback function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>function</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.callback.reset_learning_rate">
<code class="sig-prename descclassname">xgboost.callback.</code><code class="sig-name descname">reset_learning_rate</code><span class="sig-paren">(</span><em class="sig-param">learning_rates</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.callback.reset_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset learning rate after iteration 1</p>
<p>NOTE: the initial learning rate will still take in-effect on first iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>learning_rates</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em> or </em><em>function</em>) – <p>List of learning rate for each boosting round
or a customized function that calculates eta in terms of
current number of round and the total number of boosting round (e.g.
yields learning rate decay)</p>
<ul class="simple">
<li><p>list <code class="docutils literal notranslate"><span class="pre">l</span></code>: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">l[boosting_round]</span></code></p></li>
<li><p>function <code class="docutils literal notranslate"><span class="pre">f</span></code>: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">f(boosting_round,</span> <span class="pre">num_boost_round)</span></code></p></li>
</ul>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>callback</strong> – The requested callback function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>function</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.callback.early_stop">
<code class="sig-prename descclassname">xgboost.callback.</code><code class="sig-name descname">early_stop</code><span class="sig-paren">(</span><em class="sig-param">stopping_rounds</em>, <em class="sig-param">maximize=False</em>, <em class="sig-param">verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.callback.early_stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a callback that activates early stoppping.</p>
<p>Validation error needs to decrease at least
every <strong>stopping_rounds</strong> round(s) to continue training.
Requires at least one item in <strong>evals</strong>.
If there’s more than one, will use the last.
Returns the model from the last iteration (not the best one).
If early stopping occurs, the model will have three additional fields:
<code class="docutils literal notranslate"><span class="pre">bst.best_score</span></code>, <code class="docutils literal notranslate"><span class="pre">bst.best_iteration</span></code> and <code class="docutils literal notranslate"><span class="pre">bst.best_ntree_limit</span></code>.
(Use <code class="docutils literal notranslate"><span class="pre">bst.best_ntree_limit</span></code> to get the correct value if <code class="docutils literal notranslate"><span class="pre">num_parallel_tree</span></code>
and/or <code class="docutils literal notranslate"><span class="pre">num_class</span></code> appears in the parameters)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stopp_rounds</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – The stopping rounds before the trend occur.</p></li>
<li><p><strong>maximize</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to maximize evaluation metric.</p></li>
<li><p><strong>verbose</strong> (<em>optional</em><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Whether to print message about early stopping information.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>callback</strong> – The requested callback function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>function</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-xgboost.dask">
<span id="id2"></span><span id="dask-api"></span><h2>Dask API<a class="headerlink" href="python_api.html#module-xgboost.dask" title="Permalink to this headline">¶</a></h2>
<p>Dask extensions for distributed training. See xgboost/demo/dask for examples.</p>
<dl class="function">
<dt id="xgboost.dask.run">
<code class="sig-prename descclassname">xgboost.dask.</code><code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">client</em>, <em class="sig-param">func</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.dask.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Launch arbitrary function on dask workers. Workers are connected by rabit,
allowing distributed training. The environment variable OMP_NUM_THREADS is
defined on each worker according to dask - this means that calls to
xgb.train() will use the threads allocated by dask by default, unless the
user overrides the nthread parameter.</p>
<dl class="simple">
<dt>Note: Windows platforms are not officially</dt><dd><p>supported. Contributions are welcome here.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>client</strong> – Dask client representing the cluster</p></li>
<li><p><strong>func</strong> – Python function to be executed by each worker. Typically
contains xgboost training code.</p></li>
<li><p><strong>args</strong> – Arguments to be forwarded to func</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict containing the function return value for each worker</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.dask.create_worker_dmatrix">
<code class="sig-prename descclassname">xgboost.dask.</code><code class="sig-name descname">create_worker_dmatrix</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.dask.create_worker_dmatrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a DMatrix object local to a given worker. Simply forwards arguments onto the standard
DMatrix constructor, if one of the arguments is a dask dataframe, unpack the data frame to
get the local components.</p>
<p>All dask dataframe arguments must use the same partitioning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> – DMatrix constructor args.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>DMatrix object containing data local to current dask worker</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="xgboost.dask.get_local_data">
<code class="sig-prename descclassname">xgboost.dask.</code><code class="sig-name descname">get_local_data</code><span class="sig-paren">(</span><em class="sig-param">data</em><span class="sig-paren">)</span><a class="headerlink" href="python_api.html#xgboost.dask.get_local_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Unpacks a distributed data object to get the rows local to this worker</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> – A distributed dask data object</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Local data partition e.g. numpy or pandas</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


          </div>
            
  <div class="footer-relations">
    
      <div class="pull-left">
        <a class="btn btn-default" href="python_intro.html" title="previous chapter (use the left arrow)">Python Package Introduction</a>
      </div>
    
      <div class="pull-right">
        <a class="btn btn-default" href="../R-package/index.html" title="next chapter (use the right arrow)">XGBoost R Package</a>
      </div>
    </div>
    <div class="clearer"></div>
  
        </div>
        <div class="clearfix"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../R-package/index.html" title="XGBoost R Package"
             >next</a> |</li>
        <li class="right" >
          <a href="python_intro.html" title="Python Package Introduction"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">xgboost 1.0.0-SNAPSHOT documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >XGBoost Python Package</a> &#187;</li> 
      </ul>
    </div>
<script type="text/javascript">
  $("#mobile-toggle a").click(function () {
    $("#left-column").toggle();
  });
</script>
<script type="text/javascript" src="../_static/js/bootstrap.js"></script>
  <div class="footer">
    &copy; Copyright 2019, xgboost developers. Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>.
  </div>
  </body>
</html>